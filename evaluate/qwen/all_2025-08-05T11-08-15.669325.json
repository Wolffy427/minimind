{
  "results": {
    "aclue": {
      "acc_norm,none": 0.31735803463552154,
      "acc_norm_stderr,none": 0.006497107061893569,
      "acc,none": 0.31735803463552154,
      "acc_stderr,none": 0.006497107061893569,
      "alias": "aclue"
    },
    "aclue_ancient_chinese_culture": {
      "alias": " - aclue_ancient_chinese_culture",
      "acc,none": 0.4411764705882353,
      "acc_stderr,none": 0.04273430336804582,
      "acc_norm,none": 0.4411764705882353,
      "acc_norm_stderr,none": 0.04273430336804582
    },
    "aclue_ancient_literature": {
      "alias": " - aclue_ancient_literature",
      "acc,none": 0.4,
      "acc_stderr,none": 0.03885143449429057,
      "acc_norm,none": 0.4,
      "acc_norm_stderr,none": 0.03885143449429057
    },
    "aclue_ancient_medical": {
      "alias": " - aclue_ancient_medical",
      "acc,none": 0.2559241706161137,
      "acc_stderr,none": 0.03011304016776722,
      "acc_norm,none": 0.2559241706161137,
      "acc_norm_stderr,none": 0.03011304016776722
    },
    "aclue_ancient_phonetics": {
      "alias": " - aclue_ancient_phonetics",
      "acc,none": 0.3,
      "acc_stderr,none": 0.04605661864718382,
      "acc_norm,none": 0.3,
      "acc_norm_stderr,none": 0.04605661864718382
    },
    "aclue_basic_ancient_chinese": {
      "alias": " - aclue_basic_ancient_chinese",
      "acc,none": 0.24497991967871485,
      "acc_stderr,none": 0.02730980848795708,
      "acc_norm,none": 0.24497991967871485,
      "acc_norm_stderr,none": 0.02730980848795708
    },
    "aclue_couplet_prediction": {
      "alias": " - aclue_couplet_prediction",
      "acc,none": 0.342,
      "acc_stderr,none": 0.021236147199899316,
      "acc_norm,none": 0.342,
      "acc_norm_stderr,none": 0.021236147199899316
    },
    "aclue_homographic_character_resolution": {
      "alias": " - aclue_homographic_character_resolution",
      "acc,none": 0.224,
      "acc_stderr,none": 0.01866399446471086,
      "acc_norm,none": 0.224,
      "acc_norm_stderr,none": 0.01866399446471086
    },
    "aclue_named_entity_recognition": {
      "alias": " - aclue_named_entity_recognition",
      "acc,none": 0.266,
      "acc_stderr,none": 0.019780559675655396,
      "acc_norm,none": 0.266,
      "acc_norm_stderr,none": 0.019780559675655396
    },
    "aclue_poetry_appreciate": {
      "alias": " - aclue_poetry_appreciate",
      "acc,none": 0.5825242718446602,
      "acc_stderr,none": 0.04882840548212234,
      "acc_norm,none": 0.5825242718446602,
      "acc_norm_stderr,none": 0.04882840548212234
    },
    "aclue_poetry_context_prediction": {
      "alias": " - aclue_poetry_context_prediction",
      "acc,none": 0.298,
      "acc_stderr,none": 0.020475118092988954,
      "acc_norm,none": 0.298,
      "acc_norm_stderr,none": 0.020475118092988954
    },
    "aclue_poetry_quality_assessment": {
      "alias": " - aclue_poetry_quality_assessment",
      "acc,none": 0.2512315270935961,
      "acc_stderr,none": 0.021551789297996213,
      "acc_norm,none": 0.2512315270935961,
      "acc_norm_stderr,none": 0.021551789297996213
    },
    "aclue_poetry_sentiment_analysis": {
      "alias": " - aclue_poetry_sentiment_analysis",
      "acc,none": 0.308,
      "acc_stderr,none": 0.020667032987466052,
      "acc_norm,none": 0.308,
      "acc_norm_stderr,none": 0.020667032987466052
    },
    "aclue_polysemy_resolution": {
      "alias": " - aclue_polysemy_resolution",
      "acc,none": 0.272,
      "acc_stderr,none": 0.019920483209566107,
      "acc_norm,none": 0.272,
      "acc_norm_stderr,none": 0.019920483209566107
    },
    "aclue_reading_comprehension": {
      "alias": " - aclue_reading_comprehension",
      "acc,none": 0.49504950495049505,
      "acc_stderr,none": 0.049997549199812266,
      "acc_norm,none": 0.49504950495049505,
      "acc_norm_stderr,none": 0.049997549199812266
    },
    "aclue_sentence_segmentation": {
      "alias": " - aclue_sentence_segmentation",
      "acc,none": 0.48,
      "acc_stderr,none": 0.022365160424231326,
      "acc_norm,none": 0.48,
      "acc_norm_stderr,none": 0.022365160424231326
    },
    "ceval-valid": {
      "acc_norm,none": 0.39450222882615155,
      "acc_norm_stderr,none": 0.012914932871467307,
      "acc,none": 0.39450222882615155,
      "acc_stderr,none": 0.012914932871467307,
      "alias": "ceval-valid"
    },
    "ceval-valid_accountant": {
      "alias": " - ceval-valid_accountant",
      "acc,none": 0.5918367346938775,
      "acc_stderr,none": 0.07094099868916398,
      "acc_norm,none": 0.5918367346938775,
      "acc_norm_stderr,none": 0.07094099868916398
    },
    "ceval-valid_advanced_mathematics": {
      "alias": " - ceval-valid_advanced_mathematics",
      "acc,none": 0.2631578947368421,
      "acc_stderr,none": 0.10379087338771256,
      "acc_norm,none": 0.2631578947368421,
      "acc_norm_stderr,none": 0.10379087338771256
    },
    "ceval-valid_art_studies": {
      "alias": " - ceval-valid_art_studies",
      "acc,none": 0.5757575757575758,
      "acc_stderr,none": 0.08736789844447572,
      "acc_norm,none": 0.5757575757575758,
      "acc_norm_stderr,none": 0.08736789844447572
    },
    "ceval-valid_basic_medicine": {
      "alias": " - ceval-valid_basic_medicine",
      "acc,none": 0.21052631578947367,
      "acc_stderr,none": 0.09609167675529229,
      "acc_norm,none": 0.21052631578947367,
      "acc_norm_stderr,none": 0.09609167675529229
    },
    "ceval-valid_business_administration": {
      "alias": " - ceval-valid_business_administration",
      "acc,none": 0.36363636363636365,
      "acc_stderr,none": 0.08503766788122595,
      "acc_norm,none": 0.36363636363636365,
      "acc_norm_stderr,none": 0.08503766788122595
    },
    "ceval-valid_chinese_language_and_literature": {
      "alias": " - ceval-valid_chinese_language_and_literature",
      "acc,none": 0.2608695652173913,
      "acc_stderr,none": 0.09361833424764436,
      "acc_norm,none": 0.2608695652173913,
      "acc_norm_stderr,none": 0.09361833424764436
    },
    "ceval-valid_civil_servant": {
      "alias": " - ceval-valid_civil_servant",
      "acc,none": 0.5319148936170213,
      "acc_stderr,none": 0.07357064625618347,
      "acc_norm,none": 0.5319148936170213,
      "acc_norm_stderr,none": 0.07357064625618347
    },
    "ceval-valid_clinical_medicine": {
      "alias": " - ceval-valid_clinical_medicine",
      "acc,none": 0.22727272727272727,
      "acc_stderr,none": 0.09144861547306322,
      "acc_norm,none": 0.22727272727272727,
      "acc_norm_stderr,none": 0.09144861547306322
    },
    "ceval-valid_college_chemistry": {
      "alias": " - ceval-valid_college_chemistry",
      "acc,none": 0.2916666666666667,
      "acc_stderr,none": 0.09477598811252413,
      "acc_norm,none": 0.2916666666666667,
      "acc_norm_stderr,none": 0.09477598811252413
    },
    "ceval-valid_college_economics": {
      "alias": " - ceval-valid_college_economics",
      "acc,none": 0.2909090909090909,
      "acc_stderr,none": 0.061806297134458,
      "acc_norm,none": 0.2909090909090909,
      "acc_norm_stderr,none": 0.061806297134458
    },
    "ceval-valid_college_physics": {
      "alias": " - ceval-valid_college_physics",
      "acc,none": 0.3157894736842105,
      "acc_stderr,none": 0.10956136839295434,
      "acc_norm,none": 0.3157894736842105,
      "acc_norm_stderr,none": 0.10956136839295434
    },
    "ceval-valid_college_programming": {
      "alias": " - ceval-valid_college_programming",
      "acc,none": 0.2972972972972973,
      "acc_stderr,none": 0.07617808344724217,
      "acc_norm,none": 0.2972972972972973,
      "acc_norm_stderr,none": 0.07617808344724217
    },
    "ceval-valid_computer_architecture": {
      "alias": " - ceval-valid_computer_architecture",
      "acc,none": 0.3333333333333333,
      "acc_stderr,none": 0.10540925533894598,
      "acc_norm,none": 0.3333333333333333,
      "acc_norm_stderr,none": 0.10540925533894598
    },
    "ceval-valid_computer_network": {
      "alias": " - ceval-valid_computer_network",
      "acc,none": 0.2631578947368421,
      "acc_stderr,none": 0.10379087338771256,
      "acc_norm,none": 0.2631578947368421,
      "acc_norm_stderr,none": 0.10379087338771256
    },
    "ceval-valid_discrete_mathematics": {
      "alias": " - ceval-valid_discrete_mathematics",
      "acc,none": 0.4375,
      "acc_stderr,none": 0.128086884574495,
      "acc_norm,none": 0.4375,
      "acc_norm_stderr,none": 0.128086884574495
    },
    "ceval-valid_education_science": {
      "alias": " - ceval-valid_education_science",
      "acc,none": 0.3793103448275862,
      "acc_stderr,none": 0.09169709590633639,
      "acc_norm,none": 0.3793103448275862,
      "acc_norm_stderr,none": 0.09169709590633639
    },
    "ceval-valid_electrical_engineer": {
      "alias": " - ceval-valid_electrical_engineer",
      "acc,none": 0.21621621621621623,
      "acc_stderr,none": 0.06861056852129647,
      "acc_norm,none": 0.21621621621621623,
      "acc_norm_stderr,none": 0.06861056852129647
    },
    "ceval-valid_environmental_impact_assessment_engineer": {
      "alias": " - ceval-valid_environmental_impact_assessment_engineer",
      "acc,none": 0.1935483870967742,
      "acc_stderr,none": 0.07213122508063838,
      "acc_norm,none": 0.1935483870967742,
      "acc_norm_stderr,none": 0.07213122508063838
    },
    "ceval-valid_fire_engineer": {
      "alias": " - ceval-valid_fire_engineer",
      "acc,none": 0.25806451612903225,
      "acc_stderr,none": 0.0798889274021794,
      "acc_norm,none": 0.25806451612903225,
      "acc_norm_stderr,none": 0.0798889274021794
    },
    "ceval-valid_high_school_biology": {
      "alias": " - ceval-valid_high_school_biology",
      "acc,none": 0.3684210526315789,
      "acc_stderr,none": 0.1136972052352256,
      "acc_norm,none": 0.3684210526315789,
      "acc_norm_stderr,none": 0.1136972052352256
    },
    "ceval-valid_high_school_chemistry": {
      "alias": " - ceval-valid_high_school_chemistry",
      "acc,none": 0.3684210526315789,
      "acc_stderr,none": 0.1136972052352256,
      "acc_norm,none": 0.3684210526315789,
      "acc_norm_stderr,none": 0.1136972052352256
    },
    "ceval-valid_high_school_chinese": {
      "alias": " - ceval-valid_high_school_chinese",
      "acc,none": 0.5263157894736842,
      "acc_stderr,none": 0.1176877882894626,
      "acc_norm,none": 0.5263157894736842,
      "acc_norm_stderr,none": 0.1176877882894626
    },
    "ceval-valid_high_school_geography": {
      "alias": " - ceval-valid_high_school_geography",
      "acc,none": 0.3684210526315789,
      "acc_stderr,none": 0.1136972052352256,
      "acc_norm,none": 0.3684210526315789,
      "acc_norm_stderr,none": 0.1136972052352256
    },
    "ceval-valid_high_school_history": {
      "alias": " - ceval-valid_high_school_history",
      "acc,none": 0.65,
      "acc_stderr,none": 0.1094243309804831,
      "acc_norm,none": 0.65,
      "acc_norm_stderr,none": 0.1094243309804831
    },
    "ceval-valid_high_school_mathematics": {
      "alias": " - ceval-valid_high_school_mathematics",
      "acc,none": 0.2222222222222222,
      "acc_stderr,none": 0.1008316903303367,
      "acc_norm,none": 0.2222222222222222,
      "acc_norm_stderr,none": 0.1008316903303367
    },
    "ceval-valid_high_school_physics": {
      "alias": " - ceval-valid_high_school_physics",
      "acc,none": 0.5263157894736842,
      "acc_stderr,none": 0.1176877882894626,
      "acc_norm,none": 0.5263157894736842,
      "acc_norm_stderr,none": 0.1176877882894626
    },
    "ceval-valid_high_school_politics": {
      "alias": " - ceval-valid_high_school_politics",
      "acc,none": 0.6842105263157895,
      "acc_stderr,none": 0.10956136839295434,
      "acc_norm,none": 0.6842105263157895,
      "acc_norm_stderr,none": 0.10956136839295434
    },
    "ceval-valid_ideological_and_moral_cultivation": {
      "alias": " - ceval-valid_ideological_and_moral_cultivation",
      "acc,none": 0.47368421052631576,
      "acc_stderr,none": 0.11768778828946262,
      "acc_norm,none": 0.47368421052631576,
      "acc_norm_stderr,none": 0.11768778828946262
    },
    "ceval-valid_law": {
      "alias": " - ceval-valid_law",
      "acc,none": 0.25,
      "acc_stderr,none": 0.09028938981432691,
      "acc_norm,none": 0.25,
      "acc_norm_stderr,none": 0.09028938981432691
    },
    "ceval-valid_legal_professional": {
      "alias": " - ceval-valid_legal_professional",
      "acc,none": 0.21739130434782608,
      "acc_stderr,none": 0.0879391124952055,
      "acc_norm,none": 0.21739130434782608,
      "acc_norm_stderr,none": 0.0879391124952055
    },
    "ceval-valid_logic": {
      "alias": " - ceval-valid_logic",
      "acc,none": 0.4090909090909091,
      "acc_stderr,none": 0.10729033533674225,
      "acc_norm,none": 0.4090909090909091,
      "acc_norm_stderr,none": 0.10729033533674225
    },
    "ceval-valid_mao_zedong_thought": {
      "alias": " - ceval-valid_mao_zedong_thought",
      "acc,none": 0.4166666666666667,
      "acc_stderr,none": 0.10279899245732686,
      "acc_norm,none": 0.4166666666666667,
      "acc_norm_stderr,none": 0.10279899245732686
    },
    "ceval-valid_marxism": {
      "alias": " - ceval-valid_marxism",
      "acc,none": 0.3157894736842105,
      "acc_stderr,none": 0.10956136839295434,
      "acc_norm,none": 0.3157894736842105,
      "acc_norm_stderr,none": 0.10956136839295434
    },
    "ceval-valid_metrology_engineer": {
      "alias": " - ceval-valid_metrology_engineer",
      "acc,none": 0.25,
      "acc_stderr,none": 0.09028938981432691,
      "acc_norm,none": 0.25,
      "acc_norm_stderr,none": 0.09028938981432691
    },
    "ceval-valid_middle_school_biology": {
      "alias": " - ceval-valid_middle_school_biology",
      "acc,none": 0.3333333333333333,
      "acc_stderr,none": 0.10540925533894598,
      "acc_norm,none": 0.3333333333333333,
      "acc_norm_stderr,none": 0.10540925533894598
    },
    "ceval-valid_middle_school_chemistry": {
      "alias": " - ceval-valid_middle_school_chemistry",
      "acc,none": 0.55,
      "acc_stderr,none": 0.11413288653790232,
      "acc_norm,none": 0.55,
      "acc_norm_stderr,none": 0.11413288653790232
    },
    "ceval-valid_middle_school_geography": {
      "alias": " - ceval-valid_middle_school_geography",
      "acc,none": 0.16666666666666666,
      "acc_stderr,none": 0.11236664374387369,
      "acc_norm,none": 0.16666666666666666,
      "acc_norm_stderr,none": 0.11236664374387369
    },
    "ceval-valid_middle_school_history": {
      "alias": " - ceval-valid_middle_school_history",
      "acc,none": 0.6818181818181818,
      "acc_stderr,none": 0.10163945352271771,
      "acc_norm,none": 0.6818181818181818,
      "acc_norm_stderr,none": 0.10163945352271771
    },
    "ceval-valid_middle_school_mathematics": {
      "alias": " - ceval-valid_middle_school_mathematics",
      "acc,none": 0.3684210526315789,
      "acc_stderr,none": 0.1136972052352256,
      "acc_norm,none": 0.3684210526315789,
      "acc_norm_stderr,none": 0.1136972052352256
    },
    "ceval-valid_middle_school_physics": {
      "alias": " - ceval-valid_middle_school_physics",
      "acc,none": 0.6842105263157895,
      "acc_stderr,none": 0.10956136839295434,
      "acc_norm,none": 0.6842105263157895,
      "acc_norm_stderr,none": 0.10956136839295434
    },
    "ceval-valid_middle_school_politics": {
      "alias": " - ceval-valid_middle_school_politics",
      "acc,none": 0.8571428571428571,
      "acc_stderr,none": 0.07824607964359517,
      "acc_norm,none": 0.8571428571428571,
      "acc_norm_stderr,none": 0.07824607964359517
    },
    "ceval-valid_modern_chinese_history": {
      "alias": " - ceval-valid_modern_chinese_history",
      "acc,none": 0.4782608695652174,
      "acc_stderr,none": 0.10649955403405124,
      "acc_norm,none": 0.4782608695652174,
      "acc_norm_stderr,none": 0.10649955403405124
    },
    "ceval-valid_operating_system": {
      "alias": " - ceval-valid_operating_system",
      "acc,none": 0.2631578947368421,
      "acc_stderr,none": 0.10379087338771256,
      "acc_norm,none": 0.2631578947368421,
      "acc_norm_stderr,none": 0.10379087338771256
    },
    "ceval-valid_physician": {
      "alias": " - ceval-valid_physician",
      "acc,none": 0.3469387755102041,
      "acc_stderr,none": 0.06870411522695292,
      "acc_norm,none": 0.3469387755102041,
      "acc_norm_stderr,none": 0.06870411522695292
    },
    "ceval-valid_plant_protection": {
      "alias": " - ceval-valid_plant_protection",
      "acc,none": 0.36363636363636365,
      "acc_stderr,none": 0.1049727762162956,
      "acc_norm,none": 0.36363636363636365,
      "acc_norm_stderr,none": 0.1049727762162956
    },
    "ceval-valid_probability_and_statistics": {
      "alias": " - ceval-valid_probability_and_statistics",
      "acc,none": 0.16666666666666666,
      "acc_stderr,none": 0.09038769075777342,
      "acc_norm,none": 0.16666666666666666,
      "acc_norm_stderr,none": 0.09038769075777342
    },
    "ceval-valid_professional_tour_guide": {
      "alias": " - ceval-valid_professional_tour_guide",
      "acc,none": 0.3793103448275862,
      "acc_stderr,none": 0.09169709590633639,
      "acc_norm,none": 0.3793103448275862,
      "acc_norm_stderr,none": 0.09169709590633639
    },
    "ceval-valid_sports_science": {
      "alias": " - ceval-valid_sports_science",
      "acc,none": 0.3157894736842105,
      "acc_stderr,none": 0.10956136839295434,
      "acc_norm,none": 0.3157894736842105,
      "acc_norm_stderr,none": 0.10956136839295434
    },
    "ceval-valid_tax_accountant": {
      "alias": " - ceval-valid_tax_accountant",
      "acc,none": 0.5714285714285714,
      "acc_stderr,none": 0.07142857142857142,
      "acc_norm,none": 0.5714285714285714,
      "acc_norm_stderr,none": 0.07142857142857142
    },
    "ceval-valid_teacher_qualification": {
      "alias": " - ceval-valid_teacher_qualification",
      "acc,none": 0.5909090909090909,
      "acc_stderr,none": 0.07497837474124877,
      "acc_norm,none": 0.5909090909090909,
      "acc_norm_stderr,none": 0.07497837474124877
    },
    "ceval-valid_urban_and_rural_planner": {
      "alias": " - ceval-valid_urban_and_rural_planner",
      "acc,none": 0.3695652173913043,
      "acc_stderr,none": 0.07195473383945744,
      "acc_norm,none": 0.3695652173913043,
      "acc_norm_stderr,none": 0.07195473383945744
    },
    "ceval-valid_veterinary_medicine": {
      "alias": " - ceval-valid_veterinary_medicine",
      "acc,none": 0.30434782608695654,
      "acc_stderr,none": 0.09810018692482893,
      "acc_norm,none": 0.30434782608695654,
      "acc_norm_stderr,none": 0.09810018692482893
    },
    "cmmlu": {
      "acc_norm,none": 0.34113279226385773,
      "acc_norm_stderr,none": 0.004286653030555618,
      "acc,none": 0.34113279226385773,
      "acc_stderr,none": 0.004286653030555618,
      "alias": "cmmlu"
    },
    "cmmlu_agronomy": {
      "alias": " - cmmlu_agronomy",
      "acc,none": 0.2781065088757396,
      "acc_stderr,none": 0.03456905430376242,
      "acc_norm,none": 0.2781065088757396,
      "acc_norm_stderr,none": 0.03456905430376242
    },
    "cmmlu_anatomy": {
      "alias": " - cmmlu_anatomy",
      "acc,none": 0.27702702702702703,
      "acc_stderr,none": 0.03691164789738652,
      "acc_norm,none": 0.27702702702702703,
      "acc_norm_stderr,none": 0.03691164789738652
    },
    "cmmlu_ancient_chinese": {
      "alias": " - cmmlu_ancient_chinese",
      "acc,none": 0.2621951219512195,
      "acc_stderr,none": 0.034450002891734555,
      "acc_norm,none": 0.2621951219512195,
      "acc_norm_stderr,none": 0.034450002891734555
    },
    "cmmlu_arts": {
      "alias": " - cmmlu_arts",
      "acc,none": 0.38125,
      "acc_stderr,none": 0.038518021388670956,
      "acc_norm,none": 0.38125,
      "acc_norm_stderr,none": 0.038518021388670956
    },
    "cmmlu_astronomy": {
      "alias": " - cmmlu_astronomy",
      "acc,none": 0.30303030303030304,
      "acc_stderr,none": 0.035886248000917116,
      "acc_norm,none": 0.30303030303030304,
      "acc_norm_stderr,none": 0.035886248000917116
    },
    "cmmlu_business_ethics": {
      "alias": " - cmmlu_business_ethics",
      "acc,none": 0.2727272727272727,
      "acc_stderr,none": 0.030880282749398066,
      "acc_norm,none": 0.2727272727272727,
      "acc_norm_stderr,none": 0.030880282749398066
    },
    "cmmlu_chinese_civil_service_exam": {
      "alias": " - cmmlu_chinese_civil_service_exam",
      "acc,none": 0.38125,
      "acc_stderr,none": 0.038518021388670956,
      "acc_norm,none": 0.38125,
      "acc_norm_stderr,none": 0.038518021388670956
    },
    "cmmlu_chinese_driving_rule": {
      "alias": " - cmmlu_chinese_driving_rule",
      "acc,none": 0.26717557251908397,
      "acc_stderr,none": 0.03880848301082397,
      "acc_norm,none": 0.26717557251908397,
      "acc_norm_stderr,none": 0.03880848301082397
    },
    "cmmlu_chinese_food_culture": {
      "alias": " - cmmlu_chinese_food_culture",
      "acc,none": 0.4264705882352941,
      "acc_stderr,none": 0.04256528107076943,
      "acc_norm,none": 0.4264705882352941,
      "acc_norm_stderr,none": 0.04256528107076943
    },
    "cmmlu_chinese_foreign_policy": {
      "alias": " - cmmlu_chinese_foreign_policy",
      "acc,none": 0.6261682242990654,
      "acc_stderr,none": 0.046992731189948525,
      "acc_norm,none": 0.6261682242990654,
      "acc_norm_stderr,none": 0.046992731189948525
    },
    "cmmlu_chinese_history": {
      "alias": " - cmmlu_chinese_history",
      "acc,none": 0.6996904024767802,
      "acc_stderr,none": 0.025545218898401945,
      "acc_norm,none": 0.6996904024767802,
      "acc_norm_stderr,none": 0.025545218898401945
    },
    "cmmlu_chinese_literature": {
      "alias": " - cmmlu_chinese_literature",
      "acc,none": 0.39215686274509803,
      "acc_stderr,none": 0.034267123492472726,
      "acc_norm,none": 0.39215686274509803,
      "acc_norm_stderr,none": 0.034267123492472726
    },
    "cmmlu_chinese_teacher_qualification": {
      "alias": " - cmmlu_chinese_teacher_qualification",
      "acc,none": 0.4692737430167598,
      "acc_stderr,none": 0.0374057540135835,
      "acc_norm,none": 0.4692737430167598,
      "acc_norm_stderr,none": 0.0374057540135835
    },
    "cmmlu_clinical_knowledge": {
      "alias": " - cmmlu_clinical_knowledge",
      "acc,none": 0.2742616033755274,
      "acc_stderr,none": 0.029041333510597976,
      "acc_norm,none": 0.2742616033755274,
      "acc_norm_stderr,none": 0.029041333510597976
    },
    "cmmlu_college_actuarial_science": {
      "alias": " - cmmlu_college_actuarial_science",
      "acc,none": 0.25471698113207547,
      "acc_stderr,none": 0.042520162237633115,
      "acc_norm,none": 0.25471698113207547,
      "acc_norm_stderr,none": 0.042520162237633115
    },
    "cmmlu_college_education": {
      "alias": " - cmmlu_college_education",
      "acc,none": 0.4485981308411215,
      "acc_stderr,none": 0.04830698295619318,
      "acc_norm,none": 0.4485981308411215,
      "acc_norm_stderr,none": 0.04830698295619318
    },
    "cmmlu_college_engineering_hydrology": {
      "alias": " - cmmlu_college_engineering_hydrology",
      "acc,none": 0.3490566037735849,
      "acc_stderr,none": 0.046518413265290305,
      "acc_norm,none": 0.3490566037735849,
      "acc_norm_stderr,none": 0.046518413265290305
    },
    "cmmlu_college_law": {
      "alias": " - cmmlu_college_law",
      "acc,none": 0.32407407407407407,
      "acc_stderr,none": 0.04524596007030053,
      "acc_norm,none": 0.32407407407407407,
      "acc_norm_stderr,none": 0.04524596007030053
    },
    "cmmlu_college_mathematics": {
      "alias": " - cmmlu_college_mathematics",
      "acc,none": 0.2571428571428571,
      "acc_stderr,none": 0.04285714285714286,
      "acc_norm,none": 0.2571428571428571,
      "acc_norm_stderr,none": 0.04285714285714286
    },
    "cmmlu_college_medical_statistics": {
      "alias": " - cmmlu_college_medical_statistics",
      "acc,none": 0.2830188679245283,
      "acc_stderr,none": 0.04396093377439377,
      "acc_norm,none": 0.2830188679245283,
      "acc_norm_stderr,none": 0.04396093377439377
    },
    "cmmlu_college_medicine": {
      "alias": " - cmmlu_college_medicine",
      "acc,none": 0.2454212454212454,
      "acc_stderr,none": 0.02609299388422859,
      "acc_norm,none": 0.2454212454212454,
      "acc_norm_stderr,none": 0.02609299388422859
    },
    "cmmlu_computer_science": {
      "alias": " - cmmlu_computer_science",
      "acc,none": 0.30392156862745096,
      "acc_stderr,none": 0.03228210387037892,
      "acc_norm,none": 0.30392156862745096,
      "acc_norm_stderr,none": 0.03228210387037892
    },
    "cmmlu_computer_security": {
      "alias": " - cmmlu_computer_security",
      "acc,none": 0.2982456140350877,
      "acc_stderr,none": 0.03508771929824561,
      "acc_norm,none": 0.2982456140350877,
      "acc_norm_stderr,none": 0.03508771929824561
    },
    "cmmlu_conceptual_physics": {
      "alias": " - cmmlu_conceptual_physics",
      "acc,none": 0.42857142857142855,
      "acc_stderr,none": 0.04095586993435685,
      "acc_norm,none": 0.42857142857142855,
      "acc_norm_stderr,none": 0.04095586993435685
    },
    "cmmlu_construction_project_management": {
      "alias": " - cmmlu_construction_project_management",
      "acc,none": 0.26618705035971224,
      "acc_stderr,none": 0.03762240935089088,
      "acc_norm,none": 0.26618705035971224,
      "acc_norm_stderr,none": 0.03762240935089088
    },
    "cmmlu_economics": {
      "alias": " - cmmlu_economics",
      "acc,none": 0.24528301886792453,
      "acc_stderr,none": 0.03422924017644448,
      "acc_norm,none": 0.24528301886792453,
      "acc_norm_stderr,none": 0.03422924017644448
    },
    "cmmlu_education": {
      "alias": " - cmmlu_education",
      "acc,none": 0.3312883435582822,
      "acc_stderr,none": 0.03697983910025588,
      "acc_norm,none": 0.3312883435582822,
      "acc_norm_stderr,none": 0.03697983910025588
    },
    "cmmlu_electrical_engineering": {
      "alias": " - cmmlu_electrical_engineering",
      "acc,none": 0.2558139534883721,
      "acc_stderr,none": 0.03336605189761057,
      "acc_norm,none": 0.2558139534883721,
      "acc_norm_stderr,none": 0.03336605189761057
    },
    "cmmlu_elementary_chinese": {
      "alias": " - cmmlu_elementary_chinese",
      "acc,none": 0.4642857142857143,
      "acc_stderr,none": 0.031479107711218486,
      "acc_norm,none": 0.4642857142857143,
      "acc_norm_stderr,none": 0.031479107711218486
    },
    "cmmlu_elementary_commonsense": {
      "alias": " - cmmlu_elementary_commonsense",
      "acc,none": 0.29292929292929293,
      "acc_stderr,none": 0.032424979581788145,
      "acc_norm,none": 0.29292929292929293,
      "acc_norm_stderr,none": 0.032424979581788145
    },
    "cmmlu_elementary_information_and_technology": {
      "alias": " - cmmlu_elementary_information_and_technology",
      "acc,none": 0.3445378151260504,
      "acc_stderr,none": 0.030868682604121605,
      "acc_norm,none": 0.3445378151260504,
      "acc_norm_stderr,none": 0.030868682604121605
    },
    "cmmlu_elementary_mathematics": {
      "alias": " - cmmlu_elementary_mathematics",
      "acc,none": 0.34347826086956523,
      "acc_stderr,none": 0.03138025310011067,
      "acc_norm,none": 0.34347826086956523,
      "acc_norm_stderr,none": 0.03138025310011067
    },
    "cmmlu_ethnology": {
      "alias": " - cmmlu_ethnology",
      "acc,none": 0.37037037037037035,
      "acc_stderr,none": 0.0417165416135454,
      "acc_norm,none": 0.37037037037037035,
      "acc_norm_stderr,none": 0.0417165416135454
    },
    "cmmlu_food_science": {
      "alias": " - cmmlu_food_science",
      "acc,none": 0.27972027972027974,
      "acc_stderr,none": 0.03766763889539855,
      "acc_norm,none": 0.27972027972027974,
      "acc_norm_stderr,none": 0.03766763889539855
    },
    "cmmlu_genetics": {
      "alias": " - cmmlu_genetics",
      "acc,none": 0.3068181818181818,
      "acc_stderr,none": 0.03486142240553237,
      "acc_norm,none": 0.3068181818181818,
      "acc_norm_stderr,none": 0.03486142240553237
    },
    "cmmlu_global_facts": {
      "alias": " - cmmlu_global_facts",
      "acc,none": 0.2953020134228188,
      "acc_stderr,none": 0.03749763364527047,
      "acc_norm,none": 0.2953020134228188,
      "acc_norm_stderr,none": 0.03749763364527047
    },
    "cmmlu_high_school_biology": {
      "alias": " - cmmlu_high_school_biology",
      "acc,none": 0.40236686390532544,
      "acc_stderr,none": 0.037833262854165425,
      "acc_norm,none": 0.40236686390532544,
      "acc_norm_stderr,none": 0.037833262854165425
    },
    "cmmlu_high_school_chemistry": {
      "alias": " - cmmlu_high_school_chemistry",
      "acc,none": 0.5227272727272727,
      "acc_stderr,none": 0.043640050156559446,
      "acc_norm,none": 0.5227272727272727,
      "acc_norm_stderr,none": 0.043640050156559446
    },
    "cmmlu_high_school_geography": {
      "alias": " - cmmlu_high_school_geography",
      "acc,none": 0.635593220338983,
      "acc_stderr,none": 0.04449281883842519,
      "acc_norm,none": 0.635593220338983,
      "acc_norm_stderr,none": 0.04449281883842519
    },
    "cmmlu_high_school_mathematics": {
      "alias": " - cmmlu_high_school_mathematics",
      "acc,none": 0.3170731707317073,
      "acc_stderr,none": 0.03644794381282875,
      "acc_norm,none": 0.3170731707317073,
      "acc_norm_stderr,none": 0.03644794381282875
    },
    "cmmlu_high_school_physics": {
      "alias": " - cmmlu_high_school_physics",
      "acc,none": 0.5727272727272728,
      "acc_stderr,none": 0.04738198703545482,
      "acc_norm,none": 0.5727272727272728,
      "acc_norm_stderr,none": 0.04738198703545482
    },
    "cmmlu_high_school_politics": {
      "alias": " - cmmlu_high_school_politics",
      "acc,none": 0.6503496503496503,
      "acc_stderr,none": 0.0400171602838239,
      "acc_norm,none": 0.6503496503496503,
      "acc_norm_stderr,none": 0.0400171602838239
    },
    "cmmlu_human_sexuality": {
      "alias": " - cmmlu_human_sexuality",
      "acc,none": 0.25396825396825395,
      "acc_stderr,none": 0.038932596106046706,
      "acc_norm,none": 0.25396825396825395,
      "acc_norm_stderr,none": 0.038932596106046706
    },
    "cmmlu_international_law": {
      "alias": " - cmmlu_international_law",
      "acc,none": 0.2756756756756757,
      "acc_stderr,none": 0.03294252220324154,
      "acc_norm,none": 0.2756756756756757,
      "acc_norm_stderr,none": 0.03294252220324154
    },
    "cmmlu_journalism": {
      "alias": " - cmmlu_journalism",
      "acc,none": 0.26744186046511625,
      "acc_stderr,none": 0.03384836428157854,
      "acc_norm,none": 0.26744186046511625,
      "acc_norm_stderr,none": 0.03384836428157854
    },
    "cmmlu_jurisprudence": {
      "alias": " - cmmlu_jurisprudence",
      "acc,none": 0.31386861313868614,
      "acc_stderr,none": 0.02291847198462325,
      "acc_norm,none": 0.31386861313868614,
      "acc_norm_stderr,none": 0.02291847198462325
    },
    "cmmlu_legal_and_moral_basis": {
      "alias": " - cmmlu_legal_and_moral_basis",
      "acc,none": 0.4158878504672897,
      "acc_stderr,none": 0.03377119548676909,
      "acc_norm,none": 0.4158878504672897,
      "acc_norm_stderr,none": 0.03377119548676909
    },
    "cmmlu_logical": {
      "alias": " - cmmlu_logical",
      "acc,none": 0.25203252032520324,
      "acc_stderr,none": 0.03930879526823995,
      "acc_norm,none": 0.25203252032520324,
      "acc_norm_stderr,none": 0.03930879526823995
    },
    "cmmlu_machine_learning": {
      "alias": " - cmmlu_machine_learning",
      "acc,none": 0.2540983606557377,
      "acc_stderr,none": 0.03957756102798659,
      "acc_norm,none": 0.2540983606557377,
      "acc_norm_stderr,none": 0.03957756102798659
    },
    "cmmlu_management": {
      "alias": " - cmmlu_management",
      "acc,none": 0.2571428571428571,
      "acc_stderr,none": 0.03023199042074988,
      "acc_norm,none": 0.2571428571428571,
      "acc_norm_stderr,none": 0.03023199042074988
    },
    "cmmlu_marketing": {
      "alias": " - cmmlu_marketing",
      "acc,none": 0.25,
      "acc_stderr,none": 0.032364888900157734,
      "acc_norm,none": 0.25,
      "acc_norm_stderr,none": 0.032364888900157734
    },
    "cmmlu_marxist_theory": {
      "alias": " - cmmlu_marxist_theory",
      "acc,none": 0.38095238095238093,
      "acc_stderr,none": 0.03541754466656238,
      "acc_norm,none": 0.38095238095238093,
      "acc_norm_stderr,none": 0.03541754466656238
    },
    "cmmlu_modern_chinese": {
      "alias": " - cmmlu_modern_chinese",
      "acc,none": 0.3103448275862069,
      "acc_stderr,none": 0.04314091325318789,
      "acc_norm,none": 0.3103448275862069,
      "acc_norm_stderr,none": 0.04314091325318789
    },
    "cmmlu_nutrition": {
      "alias": " - cmmlu_nutrition",
      "acc,none": 0.2896551724137931,
      "acc_stderr,none": 0.03780019230438011,
      "acc_norm,none": 0.2896551724137931,
      "acc_norm_stderr,none": 0.03780019230438011
    },
    "cmmlu_philosophy": {
      "alias": " - cmmlu_philosophy",
      "acc,none": 0.3333333333333333,
      "acc_stderr,none": 0.04622501635210243,
      "acc_norm,none": 0.3333333333333333,
      "acc_norm_stderr,none": 0.04622501635210243
    },
    "cmmlu_professional_accounting": {
      "alias": " - cmmlu_professional_accounting",
      "acc,none": 0.26285714285714284,
      "acc_stderr,none": 0.03337037585221277,
      "acc_norm,none": 0.26285714285714284,
      "acc_norm_stderr,none": 0.03337037585221277
    },
    "cmmlu_professional_law": {
      "alias": " - cmmlu_professional_law",
      "acc,none": 0.3222748815165877,
      "acc_stderr,none": 0.032250048524146376,
      "acc_norm,none": 0.3222748815165877,
      "acc_norm_stderr,none": 0.032250048524146376
    },
    "cmmlu_professional_medicine": {
      "alias": " - cmmlu_professional_medicine",
      "acc,none": 0.2553191489361702,
      "acc_stderr,none": 0.02251703243459226,
      "acc_norm,none": 0.2553191489361702,
      "acc_norm_stderr,none": 0.02251703243459226
    },
    "cmmlu_professional_psychology": {
      "alias": " - cmmlu_professional_psychology",
      "acc,none": 0.33189655172413796,
      "acc_stderr,none": 0.0309825555357009,
      "acc_norm,none": 0.33189655172413796,
      "acc_norm_stderr,none": 0.0309825555357009
    },
    "cmmlu_public_relations": {
      "alias": " - cmmlu_public_relations",
      "acc,none": 0.25862068965517243,
      "acc_stderr,none": 0.03329115112144775,
      "acc_norm,none": 0.25862068965517243,
      "acc_norm_stderr,none": 0.03329115112144775
    },
    "cmmlu_security_study": {
      "alias": " - cmmlu_security_study",
      "acc,none": 0.2518518518518518,
      "acc_stderr,none": 0.03749850709174019,
      "acc_norm,none": 0.2518518518518518,
      "acc_norm_stderr,none": 0.03749850709174019
    },
    "cmmlu_sociology": {
      "alias": " - cmmlu_sociology",
      "acc,none": 0.25663716814159293,
      "acc_stderr,none": 0.029118495998237324,
      "acc_norm,none": 0.25663716814159293,
      "acc_norm_stderr,none": 0.029118495998237324
    },
    "cmmlu_sports_science": {
      "alias": " - cmmlu_sports_science",
      "acc,none": 0.2545454545454545,
      "acc_stderr,none": 0.03401506715249038,
      "acc_norm,none": 0.2545454545454545,
      "acc_norm_stderr,none": 0.03401506715249038
    },
    "cmmlu_traditional_chinese_medicine": {
      "alias": " - cmmlu_traditional_chinese_medicine",
      "acc,none": 0.24864864864864866,
      "acc_stderr,none": 0.03186439492581512,
      "acc_norm,none": 0.24864864864864866,
      "acc_norm_stderr,none": 0.03186439492581512
    },
    "cmmlu_virology": {
      "alias": " - cmmlu_virology",
      "acc,none": 0.2485207100591716,
      "acc_stderr,none": 0.033341501981019664,
      "acc_norm,none": 0.2485207100591716,
      "acc_norm_stderr,none": 0.033341501981019664
    },
    "cmmlu_world_history": {
      "alias": " - cmmlu_world_history",
      "acc,none": 0.6832298136645962,
      "acc_stderr,none": 0.03677863131157457,
      "acc_norm,none": 0.6832298136645962,
      "acc_norm_stderr,none": 0.03677863131157457
    },
    "cmmlu_world_religions": {
      "alias": " - cmmlu_world_religions",
      "acc,none": 0.3375,
      "acc_stderr,none": 0.0375,
      "acc_norm,none": 0.3375,
      "acc_norm_stderr,none": 0.0375
    },
    "tmmluplus": {
      "acc_norm,none": 0.4027281746031746,
      "acc_norm_stderr,none": 0.003411463757602825,
      "acc,none": 0.4027281746031746,
      "acc_stderr,none": 0.003411463757602825,
      "alias": "tmmluplus"
    },
    "tmmluplus_STEM": {
      "acc_norm,none": 0.40485714285714286,
      "acc_norm_stderr,none": 0.008218044538619866,
      "acc,none": 0.40485714285714286,
      "acc_stderr,none": 0.008218044538619866,
      "alias": " - tmmluplus_STEM"
    },
    "tmmluplus_advance_chemistry": {
      "alias": "  - advance chemistry",
      "acc,none": 0.4878048780487805,
      "acc_stderr,none": 0.04525440645156628,
      "acc_norm,none": 0.4878048780487805,
      "acc_norm_stderr,none": 0.04525440645156628
    },
    "tmmluplus_basic_medical_science": {
      "alias": "  - basic medical science",
      "acc,none": 0.3637316561844864,
      "acc_stderr,none": 0.015583473181027919,
      "acc_norm,none": 0.3637316561844864,
      "acc_norm_stderr,none": 0.015583473181027919
    },
    "tmmluplus_computer_science": {
      "alias": "  - computer science",
      "acc,none": 0.4885057471264368,
      "acc_stderr,none": 0.03800425000198229,
      "acc_norm,none": 0.4885057471264368,
      "acc_norm_stderr,none": 0.03800425000198229
    },
    "tmmluplus_engineering_math": {
      "alias": "  - engineering math",
      "acc,none": 0.32038834951456313,
      "acc_stderr,none": 0.046202840822800406,
      "acc_norm,none": 0.32038834951456313,
      "acc_norm_stderr,none": 0.046202840822800406
    },
    "tmmluplus_junior_chemistry": {
      "alias": "  - junior chemistry",
      "acc,none": 0.4258373205741627,
      "acc_stderr,none": 0.03428527780176408,
      "acc_norm,none": 0.4258373205741627,
      "acc_norm_stderr,none": 0.03428527780176408
    },
    "tmmluplus_junior_math_exam": {
      "alias": "  - junior math exam",
      "acc,none": 0.33714285714285713,
      "acc_stderr,none": 0.03583788137567005,
      "acc_norm,none": 0.33714285714285713,
      "acc_norm_stderr,none": 0.03583788137567005
    },
    "tmmluplus_junior_science_exam": {
      "alias": "  - junior science exam",
      "acc,none": 0.539906103286385,
      "acc_stderr,none": 0.03423059310531975,
      "acc_norm,none": 0.539906103286385,
      "acc_norm_stderr,none": 0.03423059310531975
    },
    "tmmluplus_linear_algebra": {
      "alias": "  - linear algebra",
      "acc,none": 0.3333333333333333,
      "acc_stderr,none": 0.07362101738323103,
      "acc_norm,none": 0.3333333333333333,
      "acc_norm_stderr,none": 0.07362101738323103
    },
    "tmmluplus_organic_chemistry": {
      "alias": "  - organic chemistry",
      "acc,none": 0.42201834862385323,
      "acc_stderr,none": 0.04752376228721728,
      "acc_norm,none": 0.42201834862385323,
      "acc_norm_stderr,none": 0.04752376228721728
    },
    "tmmluplus_pharmacy": {
      "alias": "  - pharmacy",
      "acc,none": 0.32225063938618925,
      "acc_stderr,none": 0.02366459894188924,
      "acc_norm,none": 0.32225063938618925,
      "acc_norm_stderr,none": 0.02366459894188924
    },
    "tmmluplus_physics": {
      "alias": "  - physics",
      "acc,none": 0.4329896907216495,
      "acc_stderr,none": 0.05057066203698089,
      "acc_norm,none": 0.4329896907216495,
      "acc_norm_stderr,none": 0.05057066203698089
    },
    "tmmluplus_secondary_physics": {
      "alias": "  - secondary physics",
      "acc,none": 0.4732142857142857,
      "acc_stderr,none": 0.047389751192741525,
      "acc_norm,none": 0.4732142857142857,
      "acc_norm_stderr,none": 0.047389751192741525
    },
    "tmmluplus_statistics_and_machine_learning": {
      "alias": "  - statistics and machine learning",
      "acc,none": 0.5223214285714286,
      "acc_stderr,none": 0.033449094836937684,
      "acc_norm,none": 0.5223214285714286,
      "acc_norm_stderr,none": 0.033449094836937684
    },
    "tmmluplus_tve_mathematics": {
      "alias": "  - tve mathematics",
      "acc,none": 0.26666666666666666,
      "acc_stderr,none": 0.036227798621918855,
      "acc_norm,none": 0.26666666666666666,
      "acc_norm_stderr,none": 0.036227798621918855
    },
    "tmmluplus_tve_natural_sciences": {
      "alias": "  - tve natural sciences",
      "acc,none": 0.45047169811320753,
      "acc_stderr,none": 0.02419126638959633,
      "acc_norm,none": 0.45047169811320753,
      "acc_norm_stderr,none": 0.02419126638959633
    },
    "tmmluplus_humanities": {
      "acc_norm,none": 0.3494044242768009,
      "acc_norm_stderr,none": 0.011340043105207608,
      "acc,none": 0.3494044242768009,
      "acc_stderr,none": 0.011340043105207608,
      "alias": " - tmmluplus_humanities"
    },
    "tmmluplus_administrative_law": {
      "alias": "  - administrative law",
      "acc,none": 0.35714285714285715,
      "acc_stderr,none": 0.023408371956384195,
      "acc_norm,none": 0.35714285714285715,
      "acc_norm_stderr,none": 0.023408371956384195
    },
    "tmmluplus_anti_money_laundering": {
      "alias": "  - anti money laundering",
      "acc,none": 0.3880597014925373,
      "acc_stderr,none": 0.04225498765506595,
      "acc_norm,none": 0.3880597014925373,
      "acc_norm_stderr,none": 0.04225498765506595
    },
    "tmmluplus_general_principles_of_law": {
      "alias": "  - general principles of law",
      "acc,none": 0.33962264150943394,
      "acc_stderr,none": 0.046216787599682604,
      "acc_norm,none": 0.33962264150943394,
      "acc_norm_stderr,none": 0.046216787599682604
    },
    "tmmluplus_introduction_to_law": {
      "alias": "  - introduction to law",
      "acc,none": 0.350210970464135,
      "acc_stderr,none": 0.031052391937584373,
      "acc_norm,none": 0.350210970464135,
      "acc_norm_stderr,none": 0.031052391937584373
    },
    "tmmluplus_jce_humanities": {
      "alias": "  - jce humanities",
      "acc,none": 0.4888888888888889,
      "acc_stderr,none": 0.05298680599073449,
      "acc_norm,none": 0.4888888888888889,
      "acc_norm_stderr,none": 0.05298680599073449
    },
    "tmmluplus_taxation": {
      "alias": "  - taxation",
      "acc,none": 0.30933333333333335,
      "acc_stderr,none": 0.023900765332555307,
      "acc_norm,none": 0.30933333333333335,
      "acc_norm_stderr,none": 0.023900765332555307
    },
    "tmmluplus_trust_practice": {
      "alias": "  - trust practice",
      "acc,none": 0.33665835411471323,
      "acc_stderr,none": 0.0236283466793323,
      "acc_norm,none": 0.33665835411471323,
      "acc_norm_stderr,none": 0.0236283466793323
    },
    "tmmluplus_other": {
      "acc_norm,none": 0.36648394675019574,
      "acc_norm_stderr,none": 0.005049901972260738,
      "acc,none": 0.36648394675019574,
      "acc_stderr,none": 0.005049901972260738,
      "alias": " - tmmluplus_other"
    },
    "tmmluplus_accounting": {
      "alias": "  - accounting",
      "acc,none": 0.33507853403141363,
      "acc_stderr,none": 0.034243778540801156,
      "acc_norm,none": 0.33507853403141363,
      "acc_norm_stderr,none": 0.034243778540801156
    },
    "tmmluplus_agriculture": {
      "alias": "  - agriculture",
      "acc,none": 0.3576158940397351,
      "acc_stderr,none": 0.0391345343117726,
      "acc_norm,none": 0.3576158940397351,
      "acc_norm_stderr,none": 0.0391345343117726
    },
    "tmmluplus_auditing": {
      "alias": "  - auditing",
      "acc,none": 0.27090909090909093,
      "acc_stderr,none": 0.018967778035178625,
      "acc_norm,none": 0.27090909090909093,
      "acc_norm_stderr,none": 0.018967778035178625
    },
    "tmmluplus_business_management": {
      "alias": "  - business management",
      "acc,none": 0.3597122302158273,
      "acc_stderr,none": 0.040853160668262056,
      "acc_norm,none": 0.3597122302158273,
      "acc_norm_stderr,none": 0.040853160668262056
    },
    "tmmluplus_culinary_skills": {
      "alias": "  - culinary skills",
      "acc,none": 0.4452054794520548,
      "acc_stderr,none": 0.02913398132351942,
      "acc_norm,none": 0.4452054794520548,
      "acc_norm_stderr,none": 0.02913398132351942
    },
    "tmmluplus_dentistry": {
      "alias": "  - dentistry",
      "acc,none": 0.3283208020050125,
      "acc_stderr,none": 0.023539032748888834,
      "acc_norm,none": 0.3283208020050125,
      "acc_norm_stderr,none": 0.023539032748888834
    },
    "tmmluplus_finance_banking": {
      "alias": "  - finance banking",
      "acc,none": 0.43703703703703706,
      "acc_stderr,none": 0.042849586397534056,
      "acc_norm,none": 0.43703703703703706,
      "acc_norm_stderr,none": 0.042849586397534056
    },
    "tmmluplus_financial_analysis": {
      "alias": "  - financial analysis",
      "acc,none": 0.4319371727748691,
      "acc_stderr,none": 0.025377333571253536,
      "acc_norm,none": 0.4319371727748691,
      "acc_norm_stderr,none": 0.025377333571253536
    },
    "tmmluplus_fire_science": {
      "alias": "  - fire science",
      "acc,none": 0.3225806451612903,
      "acc_stderr,none": 0.042149788311499216,
      "acc_norm,none": 0.3225806451612903,
      "acc_norm_stderr,none": 0.042149788311499216
    },
    "tmmluplus_insurance_studies": {
      "alias": "  - insurance studies",
      "acc,none": 0.3986842105263158,
      "acc_stderr,none": 0.017772353135495567,
      "acc_norm,none": 0.3986842105263158,
      "acc_norm_stderr,none": 0.017772353135495567
    },
    "tmmluplus_junior_social_studies": {
      "alias": "  - junior social studies",
      "acc,none": 0.6428571428571429,
      "acc_stderr,none": 0.04285714285714286,
      "acc_norm,none": 0.6428571428571429,
      "acc_norm_stderr,none": 0.04285714285714286
    },
    "tmmluplus_logic_reasoning": {
      "alias": "  - logic reasoning",
      "acc,none": 0.30935251798561153,
      "acc_stderr,none": 0.0393473511254711,
      "acc_norm,none": 0.30935251798561153,
      "acc_norm_stderr,none": 0.0393473511254711
    },
    "tmmluplus_management_accounting": {
      "alias": "  - management accounting",
      "acc,none": 0.2930232558139535,
      "acc_stderr,none": 0.03111334624875738,
      "acc_norm,none": 0.2930232558139535,
      "acc_norm_stderr,none": 0.03111334624875738
    },
    "tmmluplus_marketing_management": {
      "alias": "  - marketing management",
      "acc,none": 0.34408602150537637,
      "acc_stderr,none": 0.04952939743113241,
      "acc_norm,none": 0.34408602150537637,
      "acc_norm_stderr,none": 0.04952939743113241
    },
    "tmmluplus_mechanical": {
      "alias": "  - mechanical",
      "acc,none": 0.288135593220339,
      "acc_stderr,none": 0.04187011593049813,
      "acc_norm,none": 0.288135593220339,
      "acc_norm_stderr,none": 0.04187011593049813
    },
    "tmmluplus_music": {
      "alias": "  - music",
      "acc,none": 0.4892086330935252,
      "acc_stderr,none": 0.03003509057261219,
      "acc_norm,none": 0.4892086330935252,
      "acc_norm_stderr,none": 0.03003509057261219
    },
    "tmmluplus_nautical_science": {
      "alias": "  - nautical science",
      "acc,none": 0.2885662431941924,
      "acc_stderr,none": 0.01932006806326156,
      "acc_norm,none": 0.2885662431941924,
      "acc_norm_stderr,none": 0.01932006806326156
    },
    "tmmluplus_official_document_management": {
      "alias": "  - official document management",
      "acc,none": 0.30180180180180183,
      "acc_stderr,none": 0.030878353810993027,
      "acc_norm,none": 0.30180180180180183,
      "acc_norm_stderr,none": 0.030878353810993027
    },
    "tmmluplus_optometry": {
      "alias": "  - optometry",
      "acc,none": 0.3521739130434783,
      "acc_stderr,none": 0.01575614801384157,
      "acc_norm,none": 0.3521739130434783,
      "acc_norm_stderr,none": 0.01575614801384157
    },
    "tmmluplus_pharmacology": {
      "alias": "  - pharmacology",
      "acc,none": 0.4558058925476603,
      "acc_stderr,none": 0.020751793800748146,
      "acc_norm,none": 0.4558058925476603,
      "acc_norm_stderr,none": 0.020751793800748146
    },
    "tmmluplus_real_estate": {
      "alias": "  - real estate",
      "acc,none": 0.391304347826087,
      "acc_stderr,none": 0.05116073090561323,
      "acc_norm,none": 0.391304347826087,
      "acc_norm_stderr,none": 0.05116073090561323
    },
    "tmmluplus_technical": {
      "alias": "  - technical",
      "acc,none": 0.36069651741293535,
      "acc_stderr,none": 0.023980173634253903,
      "acc_norm,none": 0.36069651741293535,
      "acc_norm_stderr,none": 0.023980173634253903
    },
    "tmmluplus_trade": {
      "alias": "  - trade",
      "acc,none": 0.28884462151394424,
      "acc_stderr,none": 0.02024862461194513,
      "acc_norm,none": 0.28884462151394424,
      "acc_norm_stderr,none": 0.02024862461194513
    },
    "tmmluplus_traditional_chinese_medicine_clinical_medicine": {
      "alias": "  - traditional chinese medicine clinical medicine",
      "acc,none": 0.4244604316546763,
      "acc_stderr,none": 0.029697255438115598,
      "acc_norm,none": 0.4244604316546763,
      "acc_norm_stderr,none": 0.029697255438115598
    },
    "tmmluplus_tve_design": {
      "alias": "  - tve design",
      "acc,none": 0.4395833333333333,
      "acc_stderr,none": 0.02267818916624282,
      "acc_norm,none": 0.4395833333333333,
      "acc_norm_stderr,none": 0.02267818916624282
    },
    "tmmluplus_veterinary_pathology": {
      "alias": "  - veterinary pathology",
      "acc,none": 0.31802120141342755,
      "acc_stderr,none": 0.027732493478890313,
      "acc_norm,none": 0.31802120141342755,
      "acc_norm_stderr,none": 0.027732493478890313
    },
    "tmmluplus_veterinary_pharmacology": {
      "alias": "  - veterinary pharmacology",
      "acc,none": 0.34074074074074073,
      "acc_stderr,none": 0.02041483001374619,
      "acc_norm,none": 0.34074074074074073,
      "acc_norm_stderr,none": 0.02041483001374619
    },
    "tmmluplus_social_sciences": {
      "acc_norm,none": 0.47163477677072846,
      "acc_norm_stderr,none": 0.0064247912032348205,
      "acc,none": 0.47163477677072846,
      "acc_stderr,none": 0.0064247912032348205,
      "alias": " - tmmluplus_social_sciences"
    },
    "tmmluplus_chinese_language_and_literature": {
      "alias": "  - chinese language and literature",
      "acc,none": 0.40703517587939697,
      "acc_stderr,none": 0.03491385802519049,
      "acc_norm,none": 0.40703517587939697,
      "acc_norm_stderr,none": 0.03491385802519049
    },
    "tmmluplus_clinical_psychology": {
      "alias": "  - clinical psychology",
      "acc,none": 0.504,
      "acc_stderr,none": 0.04489988864128729,
      "acc_norm,none": 0.504,
      "acc_norm_stderr,none": 0.04489988864128729
    },
    "tmmluplus_economics": {
      "alias": "  - economics",
      "acc,none": 0.48346055979643765,
      "acc_stderr,none": 0.025239993315080093,
      "acc_norm,none": 0.48346055979643765,
      "acc_norm_stderr,none": 0.025239993315080093
    },
    "tmmluplus_education": {
      "alias": "  - education",
      "acc,none": 0.4596774193548387,
      "acc_stderr,none": 0.04493663917751344,
      "acc_norm,none": 0.4596774193548387,
      "acc_norm_stderr,none": 0.04493663917751344
    },
    "tmmluplus_education_(profession_level)": {
      "alias": "  - education (profession level)",
      "acc,none": 0.4382716049382716,
      "acc_stderr,none": 0.02253014499489372,
      "acc_norm,none": 0.4382716049382716,
      "acc_norm_stderr,none": 0.02253014499489372
    },
    "tmmluplus_educational_psychology": {
      "alias": "  - educational psychology",
      "acc,none": 0.5227272727272727,
      "acc_stderr,none": 0.03775738119508216,
      "acc_norm,none": 0.5227272727272727,
      "acc_norm_stderr,none": 0.03775738119508216
    },
    "tmmluplus_geography_of_taiwan": {
      "alias": "  - geography of taiwan",
      "acc,none": 0.5078125,
      "acc_stderr,none": 0.018051749630564246,
      "acc_norm,none": 0.5078125,
      "acc_norm_stderr,none": 0.018051749630564246
    },
    "tmmluplus_human_behavior": {
      "alias": "  - human behavior",
      "acc,none": 0.459546925566343,
      "acc_stderr,none": 0.028396745558962752,
      "acc_norm,none": 0.459546925566343,
      "acc_norm_stderr,none": 0.028396745558962752
    },
    "tmmluplus_junior_chinese_exam": {
      "alias": "  - junior chinese exam",
      "acc,none": 0.6628571428571428,
      "acc_stderr,none": 0.03583788137567005,
      "acc_norm,none": 0.6628571428571428,
      "acc_norm_stderr,none": 0.03583788137567005
    },
    "tmmluplus_macroeconomics": {
      "alias": "  - macroeconomics",
      "acc,none": 0.41849148418491483,
      "acc_stderr,none": 0.02436292474380383,
      "acc_norm,none": 0.41849148418491483,
      "acc_norm_stderr,none": 0.02436292474380383
    },
    "tmmluplus_national_protection": {
      "alias": "  - national protection",
      "acc,none": 0.5402843601895735,
      "acc_stderr,none": 0.03439110975404605,
      "acc_norm,none": 0.5402843601895735,
      "acc_norm_stderr,none": 0.03439110975404605
    },
    "tmmluplus_occupational_therapy_for_psychological_disorders": {
      "alias": "  - occupational therapy for psychological disorders",
      "acc,none": 0.40331491712707185,
      "acc_stderr,none": 0.021071482396527928,
      "acc_norm,none": 0.40331491712707185,
      "acc_norm_stderr,none": 0.021071482396527928
    },
    "tmmluplus_physical_education": {
      "alias": "  - physical education",
      "acc,none": 0.44692737430167595,
      "acc_stderr,none": 0.03726486555057898,
      "acc_norm,none": 0.44692737430167595,
      "acc_norm_stderr,none": 0.03726486555057898
    },
    "tmmluplus_politic_science": {
      "alias": "  - politic science",
      "acc,none": 0.4472361809045226,
      "acc_stderr,none": 0.01577048600288089,
      "acc_norm,none": 0.4472361809045226,
      "acc_norm_stderr,none": 0.01577048600288089
    },
    "tmmluplus_taiwanese_hokkien": {
      "alias": "  - taiwanese hokkien",
      "acc,none": 0.27906976744186046,
      "acc_stderr,none": 0.03964587702612167,
      "acc_norm,none": 0.27906976744186046,
      "acc_norm_stderr,none": 0.03964587702612167
    },
    "tmmluplus_three_principles_of_people": {
      "alias": "  - three principles of people",
      "acc,none": 0.5611510791366906,
      "acc_stderr,none": 0.0422433036902856,
      "acc_norm,none": 0.5611510791366906,
      "acc_norm_stderr,none": 0.0422433036902856
    },
    "tmmluplus_ttqav2": {
      "alias": "  - ttqav2",
      "acc,none": 0.49557522123893805,
      "acc_stderr,none": 0.04724370907978456,
      "acc_norm,none": 0.49557522123893805,
      "acc_norm_stderr,none": 0.04724370907978456
    },
    "tmmluplus_tve_chinese_language": {
      "alias": "  - tve chinese language",
      "acc,none": 0.5507246376811594,
      "acc_stderr,none": 0.022656876395300833,
      "acc_norm,none": 0.5507246376811594,
      "acc_norm_stderr,none": 0.022656876395300833
    }
  },
  "groups": {
    "aclue": {
      "acc_norm,none": 0.31735803463552154,
      "acc_norm_stderr,none": 0.006497107061893569,
      "acc,none": 0.31735803463552154,
      "acc_stderr,none": 0.006497107061893569,
      "alias": "aclue"
    },
    "ceval-valid": {
      "acc_norm,none": 0.39450222882615155,
      "acc_norm_stderr,none": 0.012914932871467307,
      "acc,none": 0.39450222882615155,
      "acc_stderr,none": 0.012914932871467307,
      "alias": "ceval-valid"
    },
    "cmmlu": {
      "acc_norm,none": 0.34113279226385773,
      "acc_norm_stderr,none": 0.004286653030555618,
      "acc,none": 0.34113279226385773,
      "acc_stderr,none": 0.004286653030555618,
      "alias": "cmmlu"
    },
    "tmmluplus": {
      "acc_norm,none": 0.4027281746031746,
      "acc_norm_stderr,none": 0.003411463757602825,
      "acc,none": 0.4027281746031746,
      "acc_stderr,none": 0.003411463757602825,
      "alias": "tmmluplus"
    },
    "tmmluplus_STEM": {
      "acc_norm,none": 0.40485714285714286,
      "acc_norm_stderr,none": 0.008218044538619866,
      "acc,none": 0.40485714285714286,
      "acc_stderr,none": 0.008218044538619866,
      "alias": " - tmmluplus_STEM"
    },
    "tmmluplus_humanities": {
      "acc_norm,none": 0.3494044242768009,
      "acc_norm_stderr,none": 0.011340043105207608,
      "acc,none": 0.3494044242768009,
      "acc_stderr,none": 0.011340043105207608,
      "alias": " - tmmluplus_humanities"
    },
    "tmmluplus_other": {
      "acc_norm,none": 0.36648394675019574,
      "acc_norm_stderr,none": 0.005049901972260738,
      "acc,none": 0.36648394675019574,
      "acc_stderr,none": 0.005049901972260738,
      "alias": " - tmmluplus_other"
    },
    "tmmluplus_social_sciences": {
      "acc_norm,none": 0.47163477677072846,
      "acc_norm_stderr,none": 0.0064247912032348205,
      "acc,none": 0.47163477677072846,
      "acc_stderr,none": 0.0064247912032348205,
      "alias": " - tmmluplus_social_sciences"
    }
  },
  "group_subtasks": {
    "aclue": [
      "aclue_ancient_chinese_culture",
      "aclue_ancient_literature",
      "aclue_ancient_medical",
      "aclue_ancient_phonetics",
      "aclue_basic_ancient_chinese",
      "aclue_couplet_prediction",
      "aclue_homographic_character_resolution",
      "aclue_named_entity_recognition",
      "aclue_poetry_appreciate",
      "aclue_poetry_context_prediction",
      "aclue_poetry_quality_assessment",
      "aclue_poetry_sentiment_analysis",
      "aclue_polysemy_resolution",
      "aclue_reading_comprehension",
      "aclue_sentence_segmentation"
    ],
    "ceval-valid": [
      "ceval-valid_computer_network",
      "ceval-valid_operating_system",
      "ceval-valid_computer_architecture",
      "ceval-valid_college_programming",
      "ceval-valid_college_physics",
      "ceval-valid_college_chemistry",
      "ceval-valid_advanced_mathematics",
      "ceval-valid_probability_and_statistics",
      "ceval-valid_discrete_mathematics",
      "ceval-valid_electrical_engineer",
      "ceval-valid_metrology_engineer",
      "ceval-valid_high_school_mathematics",
      "ceval-valid_high_school_physics",
      "ceval-valid_high_school_chemistry",
      "ceval-valid_high_school_biology",
      "ceval-valid_middle_school_mathematics",
      "ceval-valid_middle_school_biology",
      "ceval-valid_middle_school_physics",
      "ceval-valid_middle_school_chemistry",
      "ceval-valid_veterinary_medicine",
      "ceval-valid_college_economics",
      "ceval-valid_business_administration",
      "ceval-valid_marxism",
      "ceval-valid_mao_zedong_thought",
      "ceval-valid_education_science",
      "ceval-valid_teacher_qualification",
      "ceval-valid_high_school_politics",
      "ceval-valid_high_school_geography",
      "ceval-valid_middle_school_politics",
      "ceval-valid_middle_school_geography",
      "ceval-valid_modern_chinese_history",
      "ceval-valid_ideological_and_moral_cultivation",
      "ceval-valid_logic",
      "ceval-valid_law",
      "ceval-valid_chinese_language_and_literature",
      "ceval-valid_art_studies",
      "ceval-valid_professional_tour_guide",
      "ceval-valid_legal_professional",
      "ceval-valid_high_school_chinese",
      "ceval-valid_high_school_history",
      "ceval-valid_middle_school_history",
      "ceval-valid_civil_servant",
      "ceval-valid_sports_science",
      "ceval-valid_plant_protection",
      "ceval-valid_basic_medicine",
      "ceval-valid_clinical_medicine",
      "ceval-valid_urban_and_rural_planner",
      "ceval-valid_accountant",
      "ceval-valid_fire_engineer",
      "ceval-valid_environmental_impact_assessment_engineer",
      "ceval-valid_tax_accountant",
      "ceval-valid_physician"
    ],
    "cmmlu": [
      "cmmlu_agronomy",
      "cmmlu_anatomy",
      "cmmlu_ancient_chinese",
      "cmmlu_arts",
      "cmmlu_astronomy",
      "cmmlu_business_ethics",
      "cmmlu_chinese_civil_service_exam",
      "cmmlu_chinese_driving_rule",
      "cmmlu_chinese_food_culture",
      "cmmlu_chinese_foreign_policy",
      "cmmlu_chinese_history",
      "cmmlu_chinese_literature",
      "cmmlu_chinese_teacher_qualification",
      "cmmlu_clinical_knowledge",
      "cmmlu_college_actuarial_science",
      "cmmlu_college_education",
      "cmmlu_college_engineering_hydrology",
      "cmmlu_college_law",
      "cmmlu_college_mathematics",
      "cmmlu_college_medical_statistics",
      "cmmlu_college_medicine",
      "cmmlu_computer_science",
      "cmmlu_computer_security",
      "cmmlu_conceptual_physics",
      "cmmlu_construction_project_management",
      "cmmlu_economics",
      "cmmlu_education",
      "cmmlu_electrical_engineering",
      "cmmlu_elementary_chinese",
      "cmmlu_elementary_commonsense",
      "cmmlu_elementary_information_and_technology",
      "cmmlu_elementary_mathematics",
      "cmmlu_ethnology",
      "cmmlu_food_science",
      "cmmlu_genetics",
      "cmmlu_global_facts",
      "cmmlu_high_school_biology",
      "cmmlu_high_school_chemistry",
      "cmmlu_high_school_geography",
      "cmmlu_high_school_mathematics",
      "cmmlu_high_school_physics",
      "cmmlu_high_school_politics",
      "cmmlu_human_sexuality",
      "cmmlu_international_law",
      "cmmlu_journalism",
      "cmmlu_jurisprudence",
      "cmmlu_legal_and_moral_basis",
      "cmmlu_logical",
      "cmmlu_machine_learning",
      "cmmlu_management",
      "cmmlu_marketing",
      "cmmlu_marxist_theory",
      "cmmlu_modern_chinese",
      "cmmlu_nutrition",
      "cmmlu_philosophy",
      "cmmlu_professional_accounting",
      "cmmlu_professional_law",
      "cmmlu_professional_medicine",
      "cmmlu_professional_psychology",
      "cmmlu_public_relations",
      "cmmlu_security_study",
      "cmmlu_sociology",
      "cmmlu_sports_science",
      "cmmlu_traditional_chinese_medicine",
      "cmmlu_virology",
      "cmmlu_world_history",
      "cmmlu_world_religions"
    ],
    "tmmluplus_STEM": [
      "tmmluplus_advance_chemistry",
      "tmmluplus_basic_medical_science",
      "tmmluplus_computer_science",
      "tmmluplus_engineering_math",
      "tmmluplus_junior_chemistry",
      "tmmluplus_junior_math_exam",
      "tmmluplus_junior_science_exam",
      "tmmluplus_linear_algebra",
      "tmmluplus_organic_chemistry",
      "tmmluplus_pharmacy",
      "tmmluplus_physics",
      "tmmluplus_secondary_physics",
      "tmmluplus_statistics_and_machine_learning",
      "tmmluplus_tve_mathematics",
      "tmmluplus_tve_natural_sciences"
    ],
    "tmmluplus_humanities": [
      "tmmluplus_administrative_law",
      "tmmluplus_anti_money_laundering",
      "tmmluplus_general_principles_of_law",
      "tmmluplus_introduction_to_law",
      "tmmluplus_jce_humanities",
      "tmmluplus_taxation",
      "tmmluplus_trust_practice"
    ],
    "tmmluplus_social_sciences": [
      "tmmluplus_chinese_language_and_literature",
      "tmmluplus_clinical_psychology",
      "tmmluplus_economics",
      "tmmluplus_education",
      "tmmluplus_education_(profession_level)",
      "tmmluplus_educational_psychology",
      "tmmluplus_geography_of_taiwan",
      "tmmluplus_human_behavior",
      "tmmluplus_junior_chinese_exam",
      "tmmluplus_macroeconomics",
      "tmmluplus_national_protection",
      "tmmluplus_occupational_therapy_for_psychological_disorders",
      "tmmluplus_physical_education",
      "tmmluplus_politic_science",
      "tmmluplus_taiwanese_hokkien",
      "tmmluplus_three_principles_of_people",
      "tmmluplus_ttqav2",
      "tmmluplus_tve_chinese_language"
    ],
    "tmmluplus_other": [
      "tmmluplus_accounting",
      "tmmluplus_agriculture",
      "tmmluplus_auditing",
      "tmmluplus_business_management",
      "tmmluplus_culinary_skills",
      "tmmluplus_dentistry",
      "tmmluplus_finance_banking",
      "tmmluplus_financial_analysis",
      "tmmluplus_fire_science",
      "tmmluplus_insurance_studies",
      "tmmluplus_junior_social_studies",
      "tmmluplus_logic_reasoning",
      "tmmluplus_management_accounting",
      "tmmluplus_marketing_management",
      "tmmluplus_mechanical",
      "tmmluplus_music",
      "tmmluplus_nautical_science",
      "tmmluplus_official_document_management",
      "tmmluplus_optometry",
      "tmmluplus_pharmacology",
      "tmmluplus_real_estate",
      "tmmluplus_technical",
      "tmmluplus_trade",
      "tmmluplus_traditional_chinese_medicine_clinical_medicine",
      "tmmluplus_tve_design",
      "tmmluplus_veterinary_pathology",
      "tmmluplus_veterinary_pharmacology"
    ],
    "tmmluplus": [
      "tmmluplus_other",
      "tmmluplus_social_sciences",
      "tmmluplus_humanities",
      "tmmluplus_STEM"
    ]
  },
  "configs": {
    "aclue_ancient_chinese_culture": {
      "task": "aclue_ancient_chinese_culture",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "ancient_chinese_culture",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于国学常识的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "aclue_ancient_literature": {
      "task": "aclue_ancient_literature",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "ancient_literature",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于古代文学知识的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "aclue_ancient_medical": {
      "task": "aclue_ancient_medical",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "ancient_medical",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于医古文的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "aclue_ancient_phonetics": {
      "task": "aclue_ancient_phonetics",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "ancient_phonetics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于古音学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "aclue_basic_ancient_chinese": {
      "task": "aclue_basic_ancient_chinese",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "basic_ancient_chinese",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于古汉语知识的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "aclue_couplet_prediction": {
      "task": "aclue_couplet_prediction",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "couplet_prediction",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于对联的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "aclue_homographic_character_resolution": {
      "task": "aclue_homographic_character_resolution",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "homographic_character_resolution",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于通假字的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "aclue_named_entity_recognition": {
      "task": "aclue_named_entity_recognition",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "named_entity_recognition",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于古汉语命名体识别的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "aclue_poetry_appreciate": {
      "task": "aclue_poetry_appreciate",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "poetry_appreciate",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于古诗词曲鉴赏的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "aclue_poetry_context_prediction": {
      "task": "aclue_poetry_context_prediction",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "poetry_context_prediction",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于古诗词上下句预测的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "aclue_poetry_quality_assessment": {
      "task": "aclue_poetry_quality_assessment",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "poetry_quality_assessment",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于古诗词质量评估的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "aclue_poetry_sentiment_analysis": {
      "task": "aclue_poetry_sentiment_analysis",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "poetry_sentiment_analysis",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于诗词情感分类的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "aclue_polysemy_resolution": {
      "task": "aclue_polysemy_resolution",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "polysemy_resolution",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于古文单字多义的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "aclue_reading_comprehension": {
      "task": "aclue_reading_comprehension",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "reading_comprehension",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于古文阅读理解的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "aclue_sentence_segmentation": {
      "task": "aclue_sentence_segmentation",
      "dataset_path": "tyouisen/aclue",
      "dataset_name": "sentence_segmentation",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于古文断句的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_accountant": {
      "task": "ceval-valid_accountant",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "accountant",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于注册会计师的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_advanced_mathematics": {
      "task": "ceval-valid_advanced_mathematics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "advanced_mathematics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于高等数学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_art_studies": {
      "task": "ceval-valid_art_studies",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "art_studies",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于艺术学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_basic_medicine": {
      "task": "ceval-valid_basic_medicine",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "basic_medicine",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于基础医学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_business_administration": {
      "task": "ceval-valid_business_administration",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "business_administration",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于工商管理的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_chinese_language_and_literature": {
      "task": "ceval-valid_chinese_language_and_literature",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "chinese_language_and_literature",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于中国语言文学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_civil_servant": {
      "task": "ceval-valid_civil_servant",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "civil_servant",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于公务员的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_clinical_medicine": {
      "task": "ceval-valid_clinical_medicine",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "clinical_medicine",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于临床医学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_college_chemistry": {
      "task": "ceval-valid_college_chemistry",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "college_chemistry",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于大学化学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_college_economics": {
      "task": "ceval-valid_college_economics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "college_economics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于大学经济学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_college_physics": {
      "task": "ceval-valid_college_physics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "college_physics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于大学物理的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_college_programming": {
      "task": "ceval-valid_college_programming",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "college_programming",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于大学编程的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_computer_architecture": {
      "task": "ceval-valid_computer_architecture",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "computer_architecture",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于计算机组成的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_computer_network": {
      "task": "ceval-valid_computer_network",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "computer_network",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于计算机网络的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_discrete_mathematics": {
      "task": "ceval-valid_discrete_mathematics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "discrete_mathematics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于离散数学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_education_science": {
      "task": "ceval-valid_education_science",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "education_science",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于教育学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_electrical_engineer": {
      "task": "ceval-valid_electrical_engineer",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "electrical_engineer",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于注册电气工程师的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_environmental_impact_assessment_engineer": {
      "task": "ceval-valid_environmental_impact_assessment_engineer",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "environmental_impact_assessment_engineer",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于环境影响评价工程师的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_fire_engineer": {
      "task": "ceval-valid_fire_engineer",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "fire_engineer",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于注册消防工程师的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_high_school_biology": {
      "task": "ceval-valid_high_school_biology",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_biology",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于高中生物的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_high_school_chemistry": {
      "task": "ceval-valid_high_school_chemistry",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_chemistry",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于高中化学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_high_school_chinese": {
      "task": "ceval-valid_high_school_chinese",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_chinese",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于高中语文的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_high_school_geography": {
      "task": "ceval-valid_high_school_geography",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_geography",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于高中地理的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_high_school_history": {
      "task": "ceval-valid_high_school_history",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_history",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于高中历史的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_high_school_mathematics": {
      "task": "ceval-valid_high_school_mathematics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_mathematics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于高中数学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_high_school_physics": {
      "task": "ceval-valid_high_school_physics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_physics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于高中物理的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_high_school_politics": {
      "task": "ceval-valid_high_school_politics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "high_school_politics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于高中政治的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_ideological_and_moral_cultivation": {
      "task": "ceval-valid_ideological_and_moral_cultivation",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "ideological_and_moral_cultivation",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于思想道德修养与法律基础的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_law": {
      "task": "ceval-valid_law",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "law",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于法学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_legal_professional": {
      "task": "ceval-valid_legal_professional",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "legal_professional",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于法律职业资格的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_logic": {
      "task": "ceval-valid_logic",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "logic",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于逻辑学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_mao_zedong_thought": {
      "task": "ceval-valid_mao_zedong_thought",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "mao_zedong_thought",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于毛泽东思想和中国特色社会主义理论体系概论的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_marxism": {
      "task": "ceval-valid_marxism",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "marxism",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于马克思主义基本原理的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_metrology_engineer": {
      "task": "ceval-valid_metrology_engineer",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "metrology_engineer",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于注册计量师的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_middle_school_biology": {
      "task": "ceval-valid_middle_school_biology",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "middle_school_biology",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于初中生物的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_middle_school_chemistry": {
      "task": "ceval-valid_middle_school_chemistry",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "middle_school_chemistry",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于初中化学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_middle_school_geography": {
      "task": "ceval-valid_middle_school_geography",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "middle_school_geography",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于初中地理的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_middle_school_history": {
      "task": "ceval-valid_middle_school_history",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "middle_school_history",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于初中历史的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_middle_school_mathematics": {
      "task": "ceval-valid_middle_school_mathematics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "middle_school_mathematics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于初中数学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_middle_school_physics": {
      "task": "ceval-valid_middle_school_physics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "middle_school_physics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于初中物理的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_middle_school_politics": {
      "task": "ceval-valid_middle_school_politics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "middle_school_politics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于初中政治的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_modern_chinese_history": {
      "task": "ceval-valid_modern_chinese_history",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "modern_chinese_history",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于近代史纲要的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_operating_system": {
      "task": "ceval-valid_operating_system",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "operating_system",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于操作系统的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_physician": {
      "task": "ceval-valid_physician",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "physician",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于医师资格的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_plant_protection": {
      "task": "ceval-valid_plant_protection",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "plant_protection",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于植物保护的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_probability_and_statistics": {
      "task": "ceval-valid_probability_and_statistics",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "probability_and_statistics",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于概率统计的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_professional_tour_guide": {
      "task": "ceval-valid_professional_tour_guide",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "professional_tour_guide",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于导游资格的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_sports_science": {
      "task": "ceval-valid_sports_science",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "sports_science",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于体育学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_tax_accountant": {
      "task": "ceval-valid_tax_accountant",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "tax_accountant",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于税务师的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_teacher_qualification": {
      "task": "ceval-valid_teacher_qualification",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "teacher_qualification",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于教师资格的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_urban_and_rural_planner": {
      "task": "ceval-valid_urban_and_rural_planner",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "urban_and_rural_planner",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于注册城乡规划师的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "ceval-valid_veterinary_medicine": {
      "task": "ceval-valid_veterinary_medicine",
      "dataset_path": "ceval/ceval-exam",
      "dataset_name": "veterinary_medicine",
      "validation_split": "val",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是中国关于兽医学的单项选择题，请选出其中的正确答案。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_agronomy": {
      "task": "cmmlu_agronomy",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "agronomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于农学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_anatomy": {
      "task": "cmmlu_anatomy",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "anatomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于解剖学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_ancient_chinese": {
      "task": "cmmlu_ancient_chinese",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "ancient_chinese",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于古汉语的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_arts": {
      "task": "cmmlu_arts",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "arts",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于艺术学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_astronomy": {
      "task": "cmmlu_astronomy",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "astronomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于天文学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_business_ethics": {
      "task": "cmmlu_business_ethics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "business_ethics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于商业伦理的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_chinese_civil_service_exam": {
      "task": "cmmlu_chinese_civil_service_exam",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "chinese_civil_service_exam",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于中国公务员考试的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_chinese_driving_rule": {
      "task": "cmmlu_chinese_driving_rule",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "chinese_driving_rule",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于中国驾驶规则的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_chinese_food_culture": {
      "task": "cmmlu_chinese_food_culture",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "chinese_food_culture",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于中国饮食文化的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_chinese_foreign_policy": {
      "task": "cmmlu_chinese_foreign_policy",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "chinese_foreign_policy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于中国外交政策的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_chinese_history": {
      "task": "cmmlu_chinese_history",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "chinese_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于中国历史的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_chinese_literature": {
      "task": "cmmlu_chinese_literature",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "chinese_literature",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于中国文学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_chinese_teacher_qualification": {
      "task": "cmmlu_chinese_teacher_qualification",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "chinese_teacher_qualification",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于中国教师资格的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_clinical_knowledge": {
      "task": "cmmlu_clinical_knowledge",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "clinical_knowledge",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于临床知识的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_college_actuarial_science": {
      "task": "cmmlu_college_actuarial_science",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "college_actuarial_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于大学精算学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_college_education": {
      "task": "cmmlu_college_education",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "college_education",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于大学教育学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_college_engineering_hydrology": {
      "task": "cmmlu_college_engineering_hydrology",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "college_engineering_hydrology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于大学工程水文学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_college_law": {
      "task": "cmmlu_college_law",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "college_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于大学法律的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_college_mathematics": {
      "task": "cmmlu_college_mathematics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "college_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于大学数学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_college_medical_statistics": {
      "task": "cmmlu_college_medical_statistics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "college_medical_statistics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于大学医学统计的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_college_medicine": {
      "task": "cmmlu_college_medicine",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "college_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于大学医学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_computer_science": {
      "task": "cmmlu_computer_science",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "computer_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于计算机科学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_computer_security": {
      "task": "cmmlu_computer_security",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "computer_security",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于计算机安全的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_conceptual_physics": {
      "task": "cmmlu_conceptual_physics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "conceptual_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于概念物理学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_construction_project_management": {
      "task": "cmmlu_construction_project_management",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "construction_project_management",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于建设工程管理的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_economics": {
      "task": "cmmlu_economics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "economics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于经济学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_education": {
      "task": "cmmlu_education",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "education",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于教育学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_electrical_engineering": {
      "task": "cmmlu_electrical_engineering",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "electrical_engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于电气工程的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_elementary_chinese": {
      "task": "cmmlu_elementary_chinese",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "elementary_chinese",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于小学语文的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_elementary_commonsense": {
      "task": "cmmlu_elementary_commonsense",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "elementary_commonsense",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于小学常识的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_elementary_information_and_technology": {
      "task": "cmmlu_elementary_information_and_technology",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "elementary_information_and_technology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于小学信息技术的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_elementary_mathematics": {
      "task": "cmmlu_elementary_mathematics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "elementary_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于初等数学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_ethnology": {
      "task": "cmmlu_ethnology",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "ethnology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于民族学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_food_science": {
      "task": "cmmlu_food_science",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "food_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于食品科学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_genetics": {
      "task": "cmmlu_genetics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "genetics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于遗传学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_global_facts": {
      "task": "cmmlu_global_facts",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "global_facts",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于全球事实的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_high_school_biology": {
      "task": "cmmlu_high_school_biology",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "high_school_biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于高中生物的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_high_school_chemistry": {
      "task": "cmmlu_high_school_chemistry",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "high_school_chemistry",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于高中化学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_high_school_geography": {
      "task": "cmmlu_high_school_geography",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "high_school_geography",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于高中地理的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_high_school_mathematics": {
      "task": "cmmlu_high_school_mathematics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "high_school_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于高中数学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_high_school_physics": {
      "task": "cmmlu_high_school_physics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "high_school_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于高中物理学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_high_school_politics": {
      "task": "cmmlu_high_school_politics",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "high_school_politics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于高中政治的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_human_sexuality": {
      "task": "cmmlu_human_sexuality",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "human_sexuality",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于人类性行为的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_international_law": {
      "task": "cmmlu_international_law",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "international_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于国际法学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_journalism": {
      "task": "cmmlu_journalism",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "journalism",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于新闻学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_jurisprudence": {
      "task": "cmmlu_jurisprudence",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "jurisprudence",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于法理学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_legal_and_moral_basis": {
      "task": "cmmlu_legal_and_moral_basis",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "legal_and_moral_basis",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于法律与道德基础的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_logical": {
      "task": "cmmlu_logical",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "logical",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于逻辑学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_machine_learning": {
      "task": "cmmlu_machine_learning",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "machine_learning",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于机器学习的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_management": {
      "task": "cmmlu_management",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "management",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于管理学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_marketing": {
      "task": "cmmlu_marketing",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "marketing",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于市场营销的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_marxist_theory": {
      "task": "cmmlu_marxist_theory",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "marxist_theory",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于马克思主义理论的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_modern_chinese": {
      "task": "cmmlu_modern_chinese",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "modern_chinese",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于现代汉语的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_nutrition": {
      "task": "cmmlu_nutrition",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "nutrition",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于营养学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_philosophy": {
      "task": "cmmlu_philosophy",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "philosophy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于哲学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_professional_accounting": {
      "task": "cmmlu_professional_accounting",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "professional_accounting",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于专业会计的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_professional_law": {
      "task": "cmmlu_professional_law",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "professional_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于专业法学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_professional_medicine": {
      "task": "cmmlu_professional_medicine",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "professional_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于专业医学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_professional_psychology": {
      "task": "cmmlu_professional_psychology",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "professional_psychology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于专业心理学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_public_relations": {
      "task": "cmmlu_public_relations",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "public_relations",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于公共关系的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_security_study": {
      "task": "cmmlu_security_study",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "security_study",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于安全研究的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_sociology": {
      "task": "cmmlu_sociology",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "sociology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于社会学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_sports_science": {
      "task": "cmmlu_sports_science",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "sports_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于体育学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_traditional_chinese_medicine": {
      "task": "cmmlu_traditional_chinese_medicine",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "traditional_chinese_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于中医中药的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_virology": {
      "task": "cmmlu_virology",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "virology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于病毒学的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_world_history": {
      "task": "cmmlu_world_history",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "world_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于世界历史的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "cmmlu_world_religions": {
      "task": "cmmlu_world_religions",
      "dataset_path": "haonan-li/cmmlu",
      "dataset_name": "world_religions",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{Question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n答案：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'].index(Answer)}}",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下是关于世界宗教的单项选择题，请直接给出正确答案的选项。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 1.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_accounting": {
      "task": "tmmluplus_accounting",
      "task_alias": "accounting",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "accounting",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為會計學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_administrative_law": {
      "task": "tmmluplus_administrative_law",
      "task_alias": "administrative law",
      "tag": "tmmluplus_humanities_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "administrative_law",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為行政法的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_advance_chemistry": {
      "task": "tmmluplus_advance_chemistry",
      "task_alias": "advance chemistry",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "advance_chemistry",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為化學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_agriculture": {
      "task": "tmmluplus_agriculture",
      "task_alias": "agriculture",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "agriculture",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為農業的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_anti_money_laundering": {
      "task": "tmmluplus_anti_money_laundering",
      "task_alias": "anti money laundering",
      "tag": "tmmluplus_humanities_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "anti_money_laundering",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為洗錢防制的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_auditing": {
      "task": "tmmluplus_auditing",
      "task_alias": "auditing",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "auditing",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為審計學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_basic_medical_science": {
      "task": "tmmluplus_basic_medical_science",
      "task_alias": "basic medical science",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "basic_medical_science",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為基礎醫學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_business_management": {
      "task": "tmmluplus_business_management",
      "task_alias": "business management",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "business_management",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為企業管理的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_chinese_language_and_literature": {
      "task": "tmmluplus_chinese_language_and_literature",
      "task_alias": "chinese language and literature",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "chinese_language_and_literature",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為國文的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_clinical_psychology": {
      "task": "tmmluplus_clinical_psychology",
      "task_alias": "clinical psychology",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "clinical_psychology",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為臨床心理學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_computer_science": {
      "task": "tmmluplus_computer_science",
      "task_alias": "computer science",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "computer_science",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為資訊工程的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_culinary_skills": {
      "task": "tmmluplus_culinary_skills",
      "task_alias": "culinary skills",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "culinary_skills",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為餐旅的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_dentistry": {
      "task": "tmmluplus_dentistry",
      "task_alias": "dentistry",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "dentistry",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為牙醫學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_economics": {
      "task": "tmmluplus_economics",
      "task_alias": "economics",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "economics",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為經濟學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_education": {
      "task": "tmmluplus_education",
      "task_alias": "education",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "education",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為教育常識的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_education_(profession_level)": {
      "task": "tmmluplus_education_(profession_level)",
      "task_alias": "education (profession level)",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "education_(profession_level)",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為教育專業的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_educational_psychology": {
      "task": "tmmluplus_educational_psychology",
      "task_alias": "educational psychology",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "educational_psychology",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為教育心理的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_engineering_math": {
      "task": "tmmluplus_engineering_math",
      "task_alias": "engineering math",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "engineering_math",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為工程數學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_finance_banking": {
      "task": "tmmluplus_finance_banking",
      "task_alias": "finance banking",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "finance_banking",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為金融與法規的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_financial_analysis": {
      "task": "tmmluplus_financial_analysis",
      "task_alias": "financial analysis",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "financial_analysis",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為財務分析的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_fire_science": {
      "task": "tmmluplus_fire_science",
      "task_alias": "fire science",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "fire_science",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為火災學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_general_principles_of_law": {
      "task": "tmmluplus_general_principles_of_law",
      "task_alias": "general principles of law",
      "tag": "tmmluplus_humanities_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "general_principles_of_law",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為法學大意的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_geography_of_taiwan": {
      "task": "tmmluplus_geography_of_taiwan",
      "task_alias": "geography of taiwan",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "geography_of_taiwan",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為台灣地理的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_human_behavior": {
      "task": "tmmluplus_human_behavior",
      "task_alias": "human behavior",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "human_behavior",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為人類行為與社會的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_insurance_studies": {
      "task": "tmmluplus_insurance_studies",
      "task_alias": "insurance studies",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "insurance_studies",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為保險學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_introduction_to_law": {
      "task": "tmmluplus_introduction_to_law",
      "task_alias": "introduction to law",
      "tag": "tmmluplus_humanities_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "introduction_to_law",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為法律概論的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_jce_humanities": {
      "task": "tmmluplus_jce_humanities",
      "task_alias": "jce humanities",
      "tag": "tmmluplus_humanities_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "jce_humanities",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為指考人文科目的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_junior_chemistry": {
      "task": "tmmluplus_junior_chemistry",
      "task_alias": "junior chemistry",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "junior_chemistry",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為國中理化的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_junior_chinese_exam": {
      "task": "tmmluplus_junior_chinese_exam",
      "task_alias": "junior chinese exam",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "junior_chinese_exam",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為國中會考基測國文的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_junior_math_exam": {
      "task": "tmmluplus_junior_math_exam",
      "task_alias": "junior math exam",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "junior_math_exam",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為國中會考基測數學科的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_junior_science_exam": {
      "task": "tmmluplus_junior_science_exam",
      "task_alias": "junior science exam",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "junior_science_exam",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為國中會考基測自然科的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_junior_social_studies": {
      "task": "tmmluplus_junior_social_studies",
      "task_alias": "junior social studies",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "junior_social_studies",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為國中會考基測社會科的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_linear_algebra": {
      "task": "tmmluplus_linear_algebra",
      "task_alias": "linear algebra",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "linear_algebra",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為線代的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_logic_reasoning": {
      "task": "tmmluplus_logic_reasoning",
      "task_alias": "logic reasoning",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "logic_reasoning",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為邏輯思維的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_macroeconomics": {
      "task": "tmmluplus_macroeconomics",
      "task_alias": "macroeconomics",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "macroeconomics",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為總經的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_management_accounting": {
      "task": "tmmluplus_management_accounting",
      "task_alias": "management accounting",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "management_accounting",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為管理會計的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_marketing_management": {
      "task": "tmmluplus_marketing_management",
      "task_alias": "marketing management",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "marketing_management",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為行銷管理的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_mechanical": {
      "task": "tmmluplus_mechanical",
      "task_alias": "mechanical",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "mechanical",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為機械與機電概論的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_music": {
      "task": "tmmluplus_music",
      "task_alias": "music",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "music",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為音樂科的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_national_protection": {
      "task": "tmmluplus_national_protection",
      "task_alias": "national protection",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "national_protection",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為軍事的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_nautical_science": {
      "task": "tmmluplus_nautical_science",
      "task_alias": "nautical science",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "nautical_science",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為航海的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_occupational_therapy_for_psychological_disorders": {
      "task": "tmmluplus_occupational_therapy_for_psychological_disorders",
      "task_alias": "occupational therapy for psychological disorders",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "occupational_therapy_for_psychological_disorders",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為心理障礙職能治療學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_official_document_management": {
      "task": "tmmluplus_official_document_management",
      "task_alias": "official document management",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "official_document_management",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為機關文書的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_optometry": {
      "task": "tmmluplus_optometry",
      "task_alias": "optometry",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "optometry",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為視光學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_organic_chemistry": {
      "task": "tmmluplus_organic_chemistry",
      "task_alias": "organic chemistry",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "organic_chemistry",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為有機化學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_pharmacology": {
      "task": "tmmluplus_pharmacology",
      "task_alias": "pharmacology",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "pharmacology",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為藥理學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_pharmacy": {
      "task": "tmmluplus_pharmacy",
      "task_alias": "pharmacy",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "pharmacy",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為藥劑學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_physical_education": {
      "task": "tmmluplus_physical_education",
      "task_alias": "physical education",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "physical_education",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為體育的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_physics": {
      "task": "tmmluplus_physics",
      "task_alias": "physics",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "physics",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為物理的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_politic_science": {
      "task": "tmmluplus_politic_science",
      "task_alias": "politic science",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "politic_science",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為政治的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_real_estate": {
      "task": "tmmluplus_real_estate",
      "task_alias": "real estate",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "real_estate",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為房地產的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_secondary_physics": {
      "task": "tmmluplus_secondary_physics",
      "task_alias": "secondary physics",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "secondary_physics",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為高中物理的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_statistics_and_machine_learning": {
      "task": "tmmluplus_statistics_and_machine_learning",
      "task_alias": "statistics and machine learning",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "statistics_and_machine_learning",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為統計與機器學習的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_taiwanese_hokkien": {
      "task": "tmmluplus_taiwanese_hokkien",
      "task_alias": "taiwanese hokkien",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "taiwanese_hokkien",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為閩南語的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_taxation": {
      "task": "tmmluplus_taxation",
      "task_alias": "taxation",
      "tag": "tmmluplus_humanities_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "taxation",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為稅務的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_technical": {
      "task": "tmmluplus_technical",
      "task_alias": "technical",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "technical",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為技術工相關的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_three_principles_of_people": {
      "task": "tmmluplus_three_principles_of_people",
      "task_alias": "three principles of people",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "three_principles_of_people",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為三民主義的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_trade": {
      "task": "tmmluplus_trade",
      "task_alias": "trade",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "trade",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為貿易的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_traditional_chinese_medicine_clinical_medicine": {
      "task": "tmmluplus_traditional_chinese_medicine_clinical_medicine",
      "task_alias": "traditional chinese medicine clinical medicine",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "traditional_chinese_medicine_clinical_medicine",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為中醫臨床醫學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_trust_practice": {
      "task": "tmmluplus_trust_practice",
      "task_alias": "trust practice",
      "tag": "tmmluplus_humanities_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "trust_practice",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為信託實務的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_ttqav2": {
      "task": "tmmluplus_ttqav2",
      "task_alias": "ttqav2",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "ttqav2",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為台灣在地用語的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_tve_chinese_language": {
      "task": "tmmluplus_tve_chinese_language",
      "task_alias": "tve chinese language",
      "tag": "tmmluplus_social_sciences_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "tve_chinese_language",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為統測國文的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_tve_design": {
      "task": "tmmluplus_tve_design",
      "task_alias": "tve design",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "tve_design",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為統測 設計的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_tve_mathematics": {
      "task": "tmmluplus_tve_mathematics",
      "task_alias": "tve mathematics",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "tve_mathematics",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為統測數學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_tve_natural_sciences": {
      "task": "tmmluplus_tve_natural_sciences",
      "task_alias": "tve natural sciences",
      "tag": "tmmluplus_STEM_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "tve_natural_sciences",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為統測自然科的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_veterinary_pathology": {
      "task": "tmmluplus_veterinary_pathology",
      "task_alias": "veterinary pathology",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "veterinary_pathology",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為獸醫病理學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    },
    "tmmluplus_veterinary_pharmacology": {
      "task": "tmmluplus_veterinary_pharmacology",
      "task_alias": "veterinary pharmacology",
      "tag": "tmmluplus_other_tasks",
      "dataset_path": "ZoneTwelve/tmmluplus",
      "dataset_name": "veterinary_pharmacology",
      "test_split": "test",
      "fewshot_split": "train",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _helper(doc):\n        # modifies the contents of a single\n        # document in our dataset.\n        answer_list = [\"A\", \"B\", \"C\", \"D\"]\n        out_doc = {\n            \"questions\": doc[\"question\"],\n            \"choices\": [doc[\"A\"], doc[\"B\"], doc[\"C\"], doc[\"D\"]],\n            \"goal\": answer_list.index(doc[\"answer\"]),\n        }\n        return out_doc\n\n    return dataset.map(_helper)  # returns back a datasets.Dataset object\n",
      "doc_to_text": "{{question.strip()}}\nA. {{choices[0]}}\nB. {{choices[1]}}\nC. {{choices[2]}}\nD. {{choices[3]}}\nAnswer:",
      "doc_to_target": "answer",
      "unsafe_code": false,
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "以下為獸醫藥理學的單選題，請提供正確答案的選項。\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0,
        "pretrained": "./model/ms/Qwen/Qwen2.5-7B"
      }
    }
  },
  "versions": {
    "aclue": 1.0,
    "aclue_ancient_chinese_culture": 1.0,
    "aclue_ancient_literature": 1.0,
    "aclue_ancient_medical": 1.0,
    "aclue_ancient_phonetics": 1.0,
    "aclue_basic_ancient_chinese": 1.0,
    "aclue_couplet_prediction": 1.0,
    "aclue_homographic_character_resolution": 1.0,
    "aclue_named_entity_recognition": 1.0,
    "aclue_poetry_appreciate": 1.0,
    "aclue_poetry_context_prediction": 1.0,
    "aclue_poetry_quality_assessment": 1.0,
    "aclue_poetry_sentiment_analysis": 1.0,
    "aclue_polysemy_resolution": 1.0,
    "aclue_reading_comprehension": 1.0,
    "aclue_sentence_segmentation": 1.0,
    "ceval-valid": 2.0,
    "ceval-valid_accountant": 2.0,
    "ceval-valid_advanced_mathematics": 2.0,
    "ceval-valid_art_studies": 2.0,
    "ceval-valid_basic_medicine": 2.0,
    "ceval-valid_business_administration": 2.0,
    "ceval-valid_chinese_language_and_literature": 2.0,
    "ceval-valid_civil_servant": 2.0,
    "ceval-valid_clinical_medicine": 2.0,
    "ceval-valid_college_chemistry": 2.0,
    "ceval-valid_college_economics": 2.0,
    "ceval-valid_college_physics": 2.0,
    "ceval-valid_college_programming": 2.0,
    "ceval-valid_computer_architecture": 2.0,
    "ceval-valid_computer_network": 2.0,
    "ceval-valid_discrete_mathematics": 2.0,
    "ceval-valid_education_science": 2.0,
    "ceval-valid_electrical_engineer": 2.0,
    "ceval-valid_environmental_impact_assessment_engineer": 2.0,
    "ceval-valid_fire_engineer": 2.0,
    "ceval-valid_high_school_biology": 2.0,
    "ceval-valid_high_school_chemistry": 2.0,
    "ceval-valid_high_school_chinese": 2.0,
    "ceval-valid_high_school_geography": 2.0,
    "ceval-valid_high_school_history": 2.0,
    "ceval-valid_high_school_mathematics": 2.0,
    "ceval-valid_high_school_physics": 2.0,
    "ceval-valid_high_school_politics": 2.0,
    "ceval-valid_ideological_and_moral_cultivation": 2.0,
    "ceval-valid_law": 2.0,
    "ceval-valid_legal_professional": 2.0,
    "ceval-valid_logic": 2.0,
    "ceval-valid_mao_zedong_thought": 2.0,
    "ceval-valid_marxism": 2.0,
    "ceval-valid_metrology_engineer": 2.0,
    "ceval-valid_middle_school_biology": 2.0,
    "ceval-valid_middle_school_chemistry": 2.0,
    "ceval-valid_middle_school_geography": 2.0,
    "ceval-valid_middle_school_history": 2.0,
    "ceval-valid_middle_school_mathematics": 2.0,
    "ceval-valid_middle_school_physics": 2.0,
    "ceval-valid_middle_school_politics": 2.0,
    "ceval-valid_modern_chinese_history": 2.0,
    "ceval-valid_operating_system": 2.0,
    "ceval-valid_physician": 2.0,
    "ceval-valid_plant_protection": 2.0,
    "ceval-valid_probability_and_statistics": 2.0,
    "ceval-valid_professional_tour_guide": 2.0,
    "ceval-valid_sports_science": 2.0,
    "ceval-valid_tax_accountant": 2.0,
    "ceval-valid_teacher_qualification": 2.0,
    "ceval-valid_urban_and_rural_planner": 2.0,
    "ceval-valid_veterinary_medicine": 2.0,
    "cmmlu": 1.0,
    "cmmlu_agronomy": 1.0,
    "cmmlu_anatomy": 1.0,
    "cmmlu_ancient_chinese": 1.0,
    "cmmlu_arts": 1.0,
    "cmmlu_astronomy": 1.0,
    "cmmlu_business_ethics": 1.0,
    "cmmlu_chinese_civil_service_exam": 1.0,
    "cmmlu_chinese_driving_rule": 1.0,
    "cmmlu_chinese_food_culture": 1.0,
    "cmmlu_chinese_foreign_policy": 1.0,
    "cmmlu_chinese_history": 1.0,
    "cmmlu_chinese_literature": 1.0,
    "cmmlu_chinese_teacher_qualification": 1.0,
    "cmmlu_clinical_knowledge": 1.0,
    "cmmlu_college_actuarial_science": 1.0,
    "cmmlu_college_education": 1.0,
    "cmmlu_college_engineering_hydrology": 1.0,
    "cmmlu_college_law": 1.0,
    "cmmlu_college_mathematics": 1.0,
    "cmmlu_college_medical_statistics": 1.0,
    "cmmlu_college_medicine": 1.0,
    "cmmlu_computer_science": 1.0,
    "cmmlu_computer_security": 1.0,
    "cmmlu_conceptual_physics": 1.0,
    "cmmlu_construction_project_management": 1.0,
    "cmmlu_economics": 1.0,
    "cmmlu_education": 1.0,
    "cmmlu_electrical_engineering": 1.0,
    "cmmlu_elementary_chinese": 1.0,
    "cmmlu_elementary_commonsense": 1.0,
    "cmmlu_elementary_information_and_technology": 1.0,
    "cmmlu_elementary_mathematics": 1.0,
    "cmmlu_ethnology": 1.0,
    "cmmlu_food_science": 1.0,
    "cmmlu_genetics": 1.0,
    "cmmlu_global_facts": 1.0,
    "cmmlu_high_school_biology": 1.0,
    "cmmlu_high_school_chemistry": 1.0,
    "cmmlu_high_school_geography": 1.0,
    "cmmlu_high_school_mathematics": 1.0,
    "cmmlu_high_school_physics": 1.0,
    "cmmlu_high_school_politics": 1.0,
    "cmmlu_human_sexuality": 1.0,
    "cmmlu_international_law": 1.0,
    "cmmlu_journalism": 1.0,
    "cmmlu_jurisprudence": 1.0,
    "cmmlu_legal_and_moral_basis": 1.0,
    "cmmlu_logical": 1.0,
    "cmmlu_machine_learning": 1.0,
    "cmmlu_management": 1.0,
    "cmmlu_marketing": 1.0,
    "cmmlu_marxist_theory": 1.0,
    "cmmlu_modern_chinese": 1.0,
    "cmmlu_nutrition": 1.0,
    "cmmlu_philosophy": 1.0,
    "cmmlu_professional_accounting": 1.0,
    "cmmlu_professional_law": 1.0,
    "cmmlu_professional_medicine": 1.0,
    "cmmlu_professional_psychology": 1.0,
    "cmmlu_public_relations": 1.0,
    "cmmlu_security_study": 1.0,
    "cmmlu_sociology": 1.0,
    "cmmlu_sports_science": 1.0,
    "cmmlu_traditional_chinese_medicine": 1.0,
    "cmmlu_virology": 1.0,
    "cmmlu_world_history": 1.0,
    "cmmlu_world_religions": 1.0,
    "tmmluplus": 2.0,
    "tmmluplus_STEM": 2.0,
    "tmmluplus_accounting": 2.0,
    "tmmluplus_administrative_law": 2.0,
    "tmmluplus_advance_chemistry": 2.0,
    "tmmluplus_agriculture": 2.0,
    "tmmluplus_anti_money_laundering": 2.0,
    "tmmluplus_auditing": 2.0,
    "tmmluplus_basic_medical_science": 2.0,
    "tmmluplus_business_management": 2.0,
    "tmmluplus_chinese_language_and_literature": 2.0,
    "tmmluplus_clinical_psychology": 2.0,
    "tmmluplus_computer_science": 2.0,
    "tmmluplus_culinary_skills": 2.0,
    "tmmluplus_dentistry": 2.0,
    "tmmluplus_economics": 2.0,
    "tmmluplus_education": 2.0,
    "tmmluplus_education_(profession_level)": 2.0,
    "tmmluplus_educational_psychology": 2.0,
    "tmmluplus_engineering_math": 2.0,
    "tmmluplus_finance_banking": 2.0,
    "tmmluplus_financial_analysis": 2.0,
    "tmmluplus_fire_science": 2.0,
    "tmmluplus_general_principles_of_law": 2.0,
    "tmmluplus_geography_of_taiwan": 2.0,
    "tmmluplus_human_behavior": 2.0,
    "tmmluplus_humanities": 2.0,
    "tmmluplus_insurance_studies": 2.0,
    "tmmluplus_introduction_to_law": 2.0,
    "tmmluplus_jce_humanities": 2.0,
    "tmmluplus_junior_chemistry": 2.0,
    "tmmluplus_junior_chinese_exam": 2.0,
    "tmmluplus_junior_math_exam": 2.0,
    "tmmluplus_junior_science_exam": 2.0,
    "tmmluplus_junior_social_studies": 2.0,
    "tmmluplus_linear_algebra": 2.0,
    "tmmluplus_logic_reasoning": 2.0,
    "tmmluplus_macroeconomics": 2.0,
    "tmmluplus_management_accounting": 2.0,
    "tmmluplus_marketing_management": 2.0,
    "tmmluplus_mechanical": 2.0,
    "tmmluplus_music": 2.0,
    "tmmluplus_national_protection": 2.0,
    "tmmluplus_nautical_science": 2.0,
    "tmmluplus_occupational_therapy_for_psychological_disorders": 2.0,
    "tmmluplus_official_document_management": 2.0,
    "tmmluplus_optometry": 2.0,
    "tmmluplus_organic_chemistry": 2.0,
    "tmmluplus_other": 2.0,
    "tmmluplus_pharmacology": 2.0,
    "tmmluplus_pharmacy": 2.0,
    "tmmluplus_physical_education": 2.0,
    "tmmluplus_physics": 2.0,
    "tmmluplus_politic_science": 2.0,
    "tmmluplus_real_estate": 2.0,
    "tmmluplus_secondary_physics": 2.0,
    "tmmluplus_social_sciences": 2.0,
    "tmmluplus_statistics_and_machine_learning": 2.0,
    "tmmluplus_taiwanese_hokkien": 2.0,
    "tmmluplus_taxation": 2.0,
    "tmmluplus_technical": 2.0,
    "tmmluplus_three_principles_of_people": 2.0,
    "tmmluplus_trade": 2.0,
    "tmmluplus_traditional_chinese_medicine_clinical_medicine": 2.0,
    "tmmluplus_trust_practice": 2.0,
    "tmmluplus_ttqav2": 2.0,
    "tmmluplus_tve_chinese_language": 2.0,
    "tmmluplus_tve_design": 2.0,
    "tmmluplus_tve_mathematics": 2.0,
    "tmmluplus_tve_natural_sciences": 2.0,
    "tmmluplus_veterinary_pathology": 2.0,
    "tmmluplus_veterinary_pharmacology": 2.0
  },
  "n-shot": {
    "aclue_ancient_chinese_culture": 0,
    "aclue_ancient_literature": 0,
    "aclue_ancient_medical": 0,
    "aclue_ancient_phonetics": 0,
    "aclue_basic_ancient_chinese": 0,
    "aclue_couplet_prediction": 0,
    "aclue_homographic_character_resolution": 0,
    "aclue_named_entity_recognition": 0,
    "aclue_poetry_appreciate": 0,
    "aclue_poetry_context_prediction": 0,
    "aclue_poetry_quality_assessment": 0,
    "aclue_poetry_sentiment_analysis": 0,
    "aclue_polysemy_resolution": 0,
    "aclue_reading_comprehension": 0,
    "aclue_sentence_segmentation": 0,
    "ceval-valid_accountant": 0,
    "ceval-valid_advanced_mathematics": 0,
    "ceval-valid_art_studies": 0,
    "ceval-valid_basic_medicine": 0,
    "ceval-valid_business_administration": 0,
    "ceval-valid_chinese_language_and_literature": 0,
    "ceval-valid_civil_servant": 0,
    "ceval-valid_clinical_medicine": 0,
    "ceval-valid_college_chemistry": 0,
    "ceval-valid_college_economics": 0,
    "ceval-valid_college_physics": 0,
    "ceval-valid_college_programming": 0,
    "ceval-valid_computer_architecture": 0,
    "ceval-valid_computer_network": 0,
    "ceval-valid_discrete_mathematics": 0,
    "ceval-valid_education_science": 0,
    "ceval-valid_electrical_engineer": 0,
    "ceval-valid_environmental_impact_assessment_engineer": 0,
    "ceval-valid_fire_engineer": 0,
    "ceval-valid_high_school_biology": 0,
    "ceval-valid_high_school_chemistry": 0,
    "ceval-valid_high_school_chinese": 0,
    "ceval-valid_high_school_geography": 0,
    "ceval-valid_high_school_history": 0,
    "ceval-valid_high_school_mathematics": 0,
    "ceval-valid_high_school_physics": 0,
    "ceval-valid_high_school_politics": 0,
    "ceval-valid_ideological_and_moral_cultivation": 0,
    "ceval-valid_law": 0,
    "ceval-valid_legal_professional": 0,
    "ceval-valid_logic": 0,
    "ceval-valid_mao_zedong_thought": 0,
    "ceval-valid_marxism": 0,
    "ceval-valid_metrology_engineer": 0,
    "ceval-valid_middle_school_biology": 0,
    "ceval-valid_middle_school_chemistry": 0,
    "ceval-valid_middle_school_geography": 0,
    "ceval-valid_middle_school_history": 0,
    "ceval-valid_middle_school_mathematics": 0,
    "ceval-valid_middle_school_physics": 0,
    "ceval-valid_middle_school_politics": 0,
    "ceval-valid_modern_chinese_history": 0,
    "ceval-valid_operating_system": 0,
    "ceval-valid_physician": 0,
    "ceval-valid_plant_protection": 0,
    "ceval-valid_probability_and_statistics": 0,
    "ceval-valid_professional_tour_guide": 0,
    "ceval-valid_sports_science": 0,
    "ceval-valid_tax_accountant": 0,
    "ceval-valid_teacher_qualification": 0,
    "ceval-valid_urban_and_rural_planner": 0,
    "ceval-valid_veterinary_medicine": 0,
    "cmmlu_agronomy": 0,
    "cmmlu_anatomy": 0,
    "cmmlu_ancient_chinese": 0,
    "cmmlu_arts": 0,
    "cmmlu_astronomy": 0,
    "cmmlu_business_ethics": 0,
    "cmmlu_chinese_civil_service_exam": 0,
    "cmmlu_chinese_driving_rule": 0,
    "cmmlu_chinese_food_culture": 0,
    "cmmlu_chinese_foreign_policy": 0,
    "cmmlu_chinese_history": 0,
    "cmmlu_chinese_literature": 0,
    "cmmlu_chinese_teacher_qualification": 0,
    "cmmlu_clinical_knowledge": 0,
    "cmmlu_college_actuarial_science": 0,
    "cmmlu_college_education": 0,
    "cmmlu_college_engineering_hydrology": 0,
    "cmmlu_college_law": 0,
    "cmmlu_college_mathematics": 0,
    "cmmlu_college_medical_statistics": 0,
    "cmmlu_college_medicine": 0,
    "cmmlu_computer_science": 0,
    "cmmlu_computer_security": 0,
    "cmmlu_conceptual_physics": 0,
    "cmmlu_construction_project_management": 0,
    "cmmlu_economics": 0,
    "cmmlu_education": 0,
    "cmmlu_electrical_engineering": 0,
    "cmmlu_elementary_chinese": 0,
    "cmmlu_elementary_commonsense": 0,
    "cmmlu_elementary_information_and_technology": 0,
    "cmmlu_elementary_mathematics": 0,
    "cmmlu_ethnology": 0,
    "cmmlu_food_science": 0,
    "cmmlu_genetics": 0,
    "cmmlu_global_facts": 0,
    "cmmlu_high_school_biology": 0,
    "cmmlu_high_school_chemistry": 0,
    "cmmlu_high_school_geography": 0,
    "cmmlu_high_school_mathematics": 0,
    "cmmlu_high_school_physics": 0,
    "cmmlu_high_school_politics": 0,
    "cmmlu_human_sexuality": 0,
    "cmmlu_international_law": 0,
    "cmmlu_journalism": 0,
    "cmmlu_jurisprudence": 0,
    "cmmlu_legal_and_moral_basis": 0,
    "cmmlu_logical": 0,
    "cmmlu_machine_learning": 0,
    "cmmlu_management": 0,
    "cmmlu_marketing": 0,
    "cmmlu_marxist_theory": 0,
    "cmmlu_modern_chinese": 0,
    "cmmlu_nutrition": 0,
    "cmmlu_philosophy": 0,
    "cmmlu_professional_accounting": 0,
    "cmmlu_professional_law": 0,
    "cmmlu_professional_medicine": 0,
    "cmmlu_professional_psychology": 0,
    "cmmlu_public_relations": 0,
    "cmmlu_security_study": 0,
    "cmmlu_sociology": 0,
    "cmmlu_sports_science": 0,
    "cmmlu_traditional_chinese_medicine": 0,
    "cmmlu_virology": 0,
    "cmmlu_world_history": 0,
    "cmmlu_world_religions": 0,
    "tmmluplus_accounting": 0,
    "tmmluplus_administrative_law": 0,
    "tmmluplus_advance_chemistry": 0,
    "tmmluplus_agriculture": 0,
    "tmmluplus_anti_money_laundering": 0,
    "tmmluplus_auditing": 0,
    "tmmluplus_basic_medical_science": 0,
    "tmmluplus_business_management": 0,
    "tmmluplus_chinese_language_and_literature": 0,
    "tmmluplus_clinical_psychology": 0,
    "tmmluplus_computer_science": 0,
    "tmmluplus_culinary_skills": 0,
    "tmmluplus_dentistry": 0,
    "tmmluplus_economics": 0,
    "tmmluplus_education": 0,
    "tmmluplus_education_(profession_level)": 0,
    "tmmluplus_educational_psychology": 0,
    "tmmluplus_engineering_math": 0,
    "tmmluplus_finance_banking": 0,
    "tmmluplus_financial_analysis": 0,
    "tmmluplus_fire_science": 0,
    "tmmluplus_general_principles_of_law": 0,
    "tmmluplus_geography_of_taiwan": 0,
    "tmmluplus_human_behavior": 0,
    "tmmluplus_insurance_studies": 0,
    "tmmluplus_introduction_to_law": 0,
    "tmmluplus_jce_humanities": 0,
    "tmmluplus_junior_chemistry": 0,
    "tmmluplus_junior_chinese_exam": 0,
    "tmmluplus_junior_math_exam": 0,
    "tmmluplus_junior_science_exam": 0,
    "tmmluplus_junior_social_studies": 0,
    "tmmluplus_linear_algebra": 0,
    "tmmluplus_logic_reasoning": 0,
    "tmmluplus_macroeconomics": 0,
    "tmmluplus_management_accounting": 0,
    "tmmluplus_marketing_management": 0,
    "tmmluplus_mechanical": 0,
    "tmmluplus_music": 0,
    "tmmluplus_national_protection": 0,
    "tmmluplus_nautical_science": 0,
    "tmmluplus_occupational_therapy_for_psychological_disorders": 0,
    "tmmluplus_official_document_management": 0,
    "tmmluplus_optometry": 0,
    "tmmluplus_organic_chemistry": 0,
    "tmmluplus_pharmacology": 0,
    "tmmluplus_pharmacy": 0,
    "tmmluplus_physical_education": 0,
    "tmmluplus_physics": 0,
    "tmmluplus_politic_science": 0,
    "tmmluplus_real_estate": 0,
    "tmmluplus_secondary_physics": 0,
    "tmmluplus_statistics_and_machine_learning": 0,
    "tmmluplus_taiwanese_hokkien": 0,
    "tmmluplus_taxation": 0,
    "tmmluplus_technical": 0,
    "tmmluplus_three_principles_of_people": 0,
    "tmmluplus_trade": 0,
    "tmmluplus_traditional_chinese_medicine_clinical_medicine": 0,
    "tmmluplus_trust_practice": 0,
    "tmmluplus_ttqav2": 0,
    "tmmluplus_tve_chinese_language": 0,
    "tmmluplus_tve_design": 0,
    "tmmluplus_tve_mathematics": 0,
    "tmmluplus_tve_natural_sciences": 0,
    "tmmluplus_veterinary_pathology": 0,
    "tmmluplus_veterinary_pharmacology": 0
  },
  "higher_is_better": {
    "aclue": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_ancient_chinese_culture": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_ancient_literature": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_ancient_medical": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_ancient_phonetics": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_basic_ancient_chinese": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_couplet_prediction": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_homographic_character_resolution": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_named_entity_recognition": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_poetry_appreciate": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_poetry_context_prediction": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_poetry_quality_assessment": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_poetry_sentiment_analysis": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_polysemy_resolution": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_reading_comprehension": {
      "acc": true,
      "acc_norm": true
    },
    "aclue_sentence_segmentation": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_accountant": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_advanced_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_art_studies": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_basic_medicine": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_business_administration": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_chinese_language_and_literature": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_civil_servant": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_clinical_medicine": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_college_chemistry": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_college_economics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_college_physics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_college_programming": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_computer_architecture": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_computer_network": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_discrete_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_education_science": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_electrical_engineer": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_environmental_impact_assessment_engineer": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_fire_engineer": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_biology": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_chemistry": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_chinese": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_geography": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_history": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_physics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_high_school_politics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_ideological_and_moral_cultivation": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_law": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_legal_professional": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_logic": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_mao_zedong_thought": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_marxism": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_metrology_engineer": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_middle_school_biology": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_middle_school_chemistry": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_middle_school_geography": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_middle_school_history": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_middle_school_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_middle_school_physics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_middle_school_politics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_modern_chinese_history": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_operating_system": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_physician": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_plant_protection": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_probability_and_statistics": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_professional_tour_guide": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_sports_science": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_tax_accountant": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_teacher_qualification": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_urban_and_rural_planner": {
      "acc": true,
      "acc_norm": true
    },
    "ceval-valid_veterinary_medicine": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_agronomy": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_anatomy": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_ancient_chinese": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_arts": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_astronomy": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_business_ethics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_chinese_civil_service_exam": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_chinese_driving_rule": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_chinese_food_culture": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_chinese_foreign_policy": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_chinese_history": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_chinese_literature": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_chinese_teacher_qualification": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_clinical_knowledge": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_college_actuarial_science": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_college_education": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_college_engineering_hydrology": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_college_law": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_college_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_college_medical_statistics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_college_medicine": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_computer_science": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_computer_security": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_conceptual_physics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_construction_project_management": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_economics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_education": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_electrical_engineering": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_elementary_chinese": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_elementary_commonsense": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_elementary_information_and_technology": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_elementary_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_ethnology": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_food_science": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_genetics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_global_facts": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_high_school_biology": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_high_school_chemistry": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_high_school_geography": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_high_school_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_high_school_physics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_high_school_politics": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_human_sexuality": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_international_law": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_journalism": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_jurisprudence": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_legal_and_moral_basis": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_logical": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_machine_learning": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_management": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_marketing": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_marxist_theory": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_modern_chinese": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_nutrition": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_philosophy": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_professional_accounting": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_professional_law": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_professional_medicine": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_professional_psychology": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_public_relations": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_security_study": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_sociology": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_sports_science": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_traditional_chinese_medicine": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_virology": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_world_history": {
      "acc": true,
      "acc_norm": true
    },
    "cmmlu_world_religions": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_STEM": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_accounting": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_administrative_law": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_advance_chemistry": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_agriculture": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_anti_money_laundering": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_auditing": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_basic_medical_science": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_business_management": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_chinese_language_and_literature": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_clinical_psychology": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_computer_science": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_culinary_skills": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_dentistry": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_economics": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_education": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_education_(profession_level)": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_educational_psychology": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_engineering_math": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_finance_banking": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_financial_analysis": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_fire_science": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_general_principles_of_law": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_geography_of_taiwan": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_human_behavior": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_humanities": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_insurance_studies": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_introduction_to_law": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_jce_humanities": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_junior_chemistry": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_junior_chinese_exam": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_junior_math_exam": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_junior_science_exam": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_junior_social_studies": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_linear_algebra": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_logic_reasoning": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_macroeconomics": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_management_accounting": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_marketing_management": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_mechanical": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_music": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_national_protection": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_nautical_science": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_occupational_therapy_for_psychological_disorders": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_official_document_management": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_optometry": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_organic_chemistry": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_other": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_pharmacology": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_pharmacy": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_physical_education": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_physics": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_politic_science": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_real_estate": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_secondary_physics": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_social_sciences": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_statistics_and_machine_learning": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_taiwanese_hokkien": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_taxation": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_technical": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_three_principles_of_people": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_trade": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_traditional_chinese_medicine_clinical_medicine": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_trust_practice": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_ttqav2": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_tve_chinese_language": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_tve_design": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_tve_mathematics": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_tve_natural_sciences": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_veterinary_pathology": {
      "acc": true,
      "acc_norm": true
    },
    "tmmluplus_veterinary_pharmacology": {
      "acc": true,
      "acc_norm": true
    }
  },
  "n-samples": {
    "tmmluplus_accounting": {
      "original": 191,
      "effective": 191
    },
    "tmmluplus_agriculture": {
      "original": 151,
      "effective": 151
    },
    "tmmluplus_auditing": {
      "original": 550,
      "effective": 550
    },
    "tmmluplus_business_management": {
      "original": 139,
      "effective": 139
    },
    "tmmluplus_culinary_skills": {
      "original": 292,
      "effective": 292
    },
    "tmmluplus_dentistry": {
      "original": 399,
      "effective": 399
    },
    "tmmluplus_finance_banking": {
      "original": 135,
      "effective": 135
    },
    "tmmluplus_financial_analysis": {
      "original": 382,
      "effective": 382
    },
    "tmmluplus_fire_science": {
      "original": 124,
      "effective": 124
    },
    "tmmluplus_insurance_studies": {
      "original": 760,
      "effective": 760
    },
    "tmmluplus_junior_social_studies": {
      "original": 126,
      "effective": 126
    },
    "tmmluplus_logic_reasoning": {
      "original": 139,
      "effective": 139
    },
    "tmmluplus_management_accounting": {
      "original": 215,
      "effective": 215
    },
    "tmmluplus_marketing_management": {
      "original": 93,
      "effective": 93
    },
    "tmmluplus_mechanical": {
      "original": 118,
      "effective": 118
    },
    "tmmluplus_music": {
      "original": 278,
      "effective": 278
    },
    "tmmluplus_nautical_science": {
      "original": 551,
      "effective": 551
    },
    "tmmluplus_official_document_management": {
      "original": 222,
      "effective": 222
    },
    "tmmluplus_optometry": {
      "original": 920,
      "effective": 920
    },
    "tmmluplus_pharmacology": {
      "original": 577,
      "effective": 577
    },
    "tmmluplus_real_estate": {
      "original": 92,
      "effective": 92
    },
    "tmmluplus_technical": {
      "original": 402,
      "effective": 402
    },
    "tmmluplus_trade": {
      "original": 502,
      "effective": 502
    },
    "tmmluplus_traditional_chinese_medicine_clinical_medicine": {
      "original": 278,
      "effective": 278
    },
    "tmmluplus_tve_design": {
      "original": 480,
      "effective": 480
    },
    "tmmluplus_veterinary_pathology": {
      "original": 283,
      "effective": 283
    },
    "tmmluplus_veterinary_pharmacology": {
      "original": 540,
      "effective": 540
    },
    "tmmluplus_chinese_language_and_literature": {
      "original": 199,
      "effective": 199
    },
    "tmmluplus_clinical_psychology": {
      "original": 125,
      "effective": 125
    },
    "tmmluplus_economics": {
      "original": 393,
      "effective": 393
    },
    "tmmluplus_education": {
      "original": 124,
      "effective": 124
    },
    "tmmluplus_education_(profession_level)": {
      "original": 486,
      "effective": 486
    },
    "tmmluplus_educational_psychology": {
      "original": 176,
      "effective": 176
    },
    "tmmluplus_geography_of_taiwan": {
      "original": 768,
      "effective": 768
    },
    "tmmluplus_human_behavior": {
      "original": 309,
      "effective": 309
    },
    "tmmluplus_junior_chinese_exam": {
      "original": 175,
      "effective": 175
    },
    "tmmluplus_macroeconomics": {
      "original": 411,
      "effective": 411
    },
    "tmmluplus_national_protection": {
      "original": 211,
      "effective": 211
    },
    "tmmluplus_occupational_therapy_for_psychological_disorders": {
      "original": 543,
      "effective": 543
    },
    "tmmluplus_physical_education": {
      "original": 179,
      "effective": 179
    },
    "tmmluplus_politic_science": {
      "original": 995,
      "effective": 995
    },
    "tmmluplus_taiwanese_hokkien": {
      "original": 129,
      "effective": 129
    },
    "tmmluplus_three_principles_of_people": {
      "original": 139,
      "effective": 139
    },
    "tmmluplus_ttqav2": {
      "original": 113,
      "effective": 113
    },
    "tmmluplus_tve_chinese_language": {
      "original": 483,
      "effective": 483
    },
    "tmmluplus_administrative_law": {
      "original": 420,
      "effective": 420
    },
    "tmmluplus_anti_money_laundering": {
      "original": 134,
      "effective": 134
    },
    "tmmluplus_general_principles_of_law": {
      "original": 106,
      "effective": 106
    },
    "tmmluplus_introduction_to_law": {
      "original": 237,
      "effective": 237
    },
    "tmmluplus_jce_humanities": {
      "original": 90,
      "effective": 90
    },
    "tmmluplus_taxation": {
      "original": 375,
      "effective": 375
    },
    "tmmluplus_trust_practice": {
      "original": 401,
      "effective": 401
    },
    "tmmluplus_advance_chemistry": {
      "original": 123,
      "effective": 123
    },
    "tmmluplus_basic_medical_science": {
      "original": 954,
      "effective": 954
    },
    "tmmluplus_computer_science": {
      "original": 174,
      "effective": 174
    },
    "tmmluplus_engineering_math": {
      "original": 103,
      "effective": 103
    },
    "tmmluplus_junior_chemistry": {
      "original": 209,
      "effective": 209
    },
    "tmmluplus_junior_math_exam": {
      "original": 175,
      "effective": 175
    },
    "tmmluplus_junior_science_exam": {
      "original": 213,
      "effective": 213
    },
    "tmmluplus_linear_algebra": {
      "original": 42,
      "effective": 42
    },
    "tmmluplus_organic_chemistry": {
      "original": 109,
      "effective": 109
    },
    "tmmluplus_pharmacy": {
      "original": 391,
      "effective": 391
    },
    "tmmluplus_physics": {
      "original": 97,
      "effective": 97
    },
    "tmmluplus_secondary_physics": {
      "original": 112,
      "effective": 112
    },
    "tmmluplus_statistics_and_machine_learning": {
      "original": 224,
      "effective": 224
    },
    "tmmluplus_tve_mathematics": {
      "original": 150,
      "effective": 150
    },
    "tmmluplus_tve_natural_sciences": {
      "original": 424,
      "effective": 424
    },
    "cmmlu_agronomy": {
      "original": 169,
      "effective": 169
    },
    "cmmlu_anatomy": {
      "original": 148,
      "effective": 148
    },
    "cmmlu_ancient_chinese": {
      "original": 164,
      "effective": 164
    },
    "cmmlu_arts": {
      "original": 160,
      "effective": 160
    },
    "cmmlu_astronomy": {
      "original": 165,
      "effective": 165
    },
    "cmmlu_business_ethics": {
      "original": 209,
      "effective": 209
    },
    "cmmlu_chinese_civil_service_exam": {
      "original": 160,
      "effective": 160
    },
    "cmmlu_chinese_driving_rule": {
      "original": 131,
      "effective": 131
    },
    "cmmlu_chinese_food_culture": {
      "original": 136,
      "effective": 136
    },
    "cmmlu_chinese_foreign_policy": {
      "original": 107,
      "effective": 107
    },
    "cmmlu_chinese_history": {
      "original": 323,
      "effective": 323
    },
    "cmmlu_chinese_literature": {
      "original": 204,
      "effective": 204
    },
    "cmmlu_chinese_teacher_qualification": {
      "original": 179,
      "effective": 179
    },
    "cmmlu_clinical_knowledge": {
      "original": 237,
      "effective": 237
    },
    "cmmlu_college_actuarial_science": {
      "original": 106,
      "effective": 106
    },
    "cmmlu_college_education": {
      "original": 107,
      "effective": 107
    },
    "cmmlu_college_engineering_hydrology": {
      "original": 106,
      "effective": 106
    },
    "cmmlu_college_law": {
      "original": 108,
      "effective": 108
    },
    "cmmlu_college_mathematics": {
      "original": 105,
      "effective": 105
    },
    "cmmlu_college_medical_statistics": {
      "original": 106,
      "effective": 106
    },
    "cmmlu_college_medicine": {
      "original": 273,
      "effective": 273
    },
    "cmmlu_computer_science": {
      "original": 204,
      "effective": 204
    },
    "cmmlu_computer_security": {
      "original": 171,
      "effective": 171
    },
    "cmmlu_conceptual_physics": {
      "original": 147,
      "effective": 147
    },
    "cmmlu_construction_project_management": {
      "original": 139,
      "effective": 139
    },
    "cmmlu_economics": {
      "original": 159,
      "effective": 159
    },
    "cmmlu_education": {
      "original": 163,
      "effective": 163
    },
    "cmmlu_electrical_engineering": {
      "original": 172,
      "effective": 172
    },
    "cmmlu_elementary_chinese": {
      "original": 252,
      "effective": 252
    },
    "cmmlu_elementary_commonsense": {
      "original": 198,
      "effective": 198
    },
    "cmmlu_elementary_information_and_technology": {
      "original": 238,
      "effective": 238
    },
    "cmmlu_elementary_mathematics": {
      "original": 230,
      "effective": 230
    },
    "cmmlu_ethnology": {
      "original": 135,
      "effective": 135
    },
    "cmmlu_food_science": {
      "original": 143,
      "effective": 143
    },
    "cmmlu_genetics": {
      "original": 176,
      "effective": 176
    },
    "cmmlu_global_facts": {
      "original": 149,
      "effective": 149
    },
    "cmmlu_high_school_biology": {
      "original": 169,
      "effective": 169
    },
    "cmmlu_high_school_chemistry": {
      "original": 132,
      "effective": 132
    },
    "cmmlu_high_school_geography": {
      "original": 118,
      "effective": 118
    },
    "cmmlu_high_school_mathematics": {
      "original": 164,
      "effective": 164
    },
    "cmmlu_high_school_physics": {
      "original": 110,
      "effective": 110
    },
    "cmmlu_high_school_politics": {
      "original": 143,
      "effective": 143
    },
    "cmmlu_human_sexuality": {
      "original": 126,
      "effective": 126
    },
    "cmmlu_international_law": {
      "original": 185,
      "effective": 185
    },
    "cmmlu_journalism": {
      "original": 172,
      "effective": 172
    },
    "cmmlu_jurisprudence": {
      "original": 411,
      "effective": 411
    },
    "cmmlu_legal_and_moral_basis": {
      "original": 214,
      "effective": 214
    },
    "cmmlu_logical": {
      "original": 123,
      "effective": 123
    },
    "cmmlu_machine_learning": {
      "original": 122,
      "effective": 122
    },
    "cmmlu_management": {
      "original": 210,
      "effective": 210
    },
    "cmmlu_marketing": {
      "original": 180,
      "effective": 180
    },
    "cmmlu_marxist_theory": {
      "original": 189,
      "effective": 189
    },
    "cmmlu_modern_chinese": {
      "original": 116,
      "effective": 116
    },
    "cmmlu_nutrition": {
      "original": 145,
      "effective": 145
    },
    "cmmlu_philosophy": {
      "original": 105,
      "effective": 105
    },
    "cmmlu_professional_accounting": {
      "original": 175,
      "effective": 175
    },
    "cmmlu_professional_law": {
      "original": 211,
      "effective": 211
    },
    "cmmlu_professional_medicine": {
      "original": 376,
      "effective": 376
    },
    "cmmlu_professional_psychology": {
      "original": 232,
      "effective": 232
    },
    "cmmlu_public_relations": {
      "original": 174,
      "effective": 174
    },
    "cmmlu_security_study": {
      "original": 135,
      "effective": 135
    },
    "cmmlu_sociology": {
      "original": 226,
      "effective": 226
    },
    "cmmlu_sports_science": {
      "original": 165,
      "effective": 165
    },
    "cmmlu_traditional_chinese_medicine": {
      "original": 185,
      "effective": 185
    },
    "cmmlu_virology": {
      "original": 169,
      "effective": 169
    },
    "cmmlu_world_history": {
      "original": 161,
      "effective": 161
    },
    "cmmlu_world_religions": {
      "original": 160,
      "effective": 160
    },
    "ceval-valid_computer_network": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_operating_system": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_computer_architecture": {
      "original": 21,
      "effective": 21
    },
    "ceval-valid_college_programming": {
      "original": 37,
      "effective": 37
    },
    "ceval-valid_college_physics": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_college_chemistry": {
      "original": 24,
      "effective": 24
    },
    "ceval-valid_advanced_mathematics": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_probability_and_statistics": {
      "original": 18,
      "effective": 18
    },
    "ceval-valid_discrete_mathematics": {
      "original": 16,
      "effective": 16
    },
    "ceval-valid_electrical_engineer": {
      "original": 37,
      "effective": 37
    },
    "ceval-valid_metrology_engineer": {
      "original": 24,
      "effective": 24
    },
    "ceval-valid_high_school_mathematics": {
      "original": 18,
      "effective": 18
    },
    "ceval-valid_high_school_physics": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_high_school_chemistry": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_high_school_biology": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_middle_school_mathematics": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_middle_school_biology": {
      "original": 21,
      "effective": 21
    },
    "ceval-valid_middle_school_physics": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_middle_school_chemistry": {
      "original": 20,
      "effective": 20
    },
    "ceval-valid_veterinary_medicine": {
      "original": 23,
      "effective": 23
    },
    "ceval-valid_college_economics": {
      "original": 55,
      "effective": 55
    },
    "ceval-valid_business_administration": {
      "original": 33,
      "effective": 33
    },
    "ceval-valid_marxism": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_mao_zedong_thought": {
      "original": 24,
      "effective": 24
    },
    "ceval-valid_education_science": {
      "original": 29,
      "effective": 29
    },
    "ceval-valid_teacher_qualification": {
      "original": 44,
      "effective": 44
    },
    "ceval-valid_high_school_politics": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_high_school_geography": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_middle_school_politics": {
      "original": 21,
      "effective": 21
    },
    "ceval-valid_middle_school_geography": {
      "original": 12,
      "effective": 12
    },
    "ceval-valid_modern_chinese_history": {
      "original": 23,
      "effective": 23
    },
    "ceval-valid_ideological_and_moral_cultivation": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_logic": {
      "original": 22,
      "effective": 22
    },
    "ceval-valid_law": {
      "original": 24,
      "effective": 24
    },
    "ceval-valid_chinese_language_and_literature": {
      "original": 23,
      "effective": 23
    },
    "ceval-valid_art_studies": {
      "original": 33,
      "effective": 33
    },
    "ceval-valid_professional_tour_guide": {
      "original": 29,
      "effective": 29
    },
    "ceval-valid_legal_professional": {
      "original": 23,
      "effective": 23
    },
    "ceval-valid_high_school_chinese": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_high_school_history": {
      "original": 20,
      "effective": 20
    },
    "ceval-valid_middle_school_history": {
      "original": 22,
      "effective": 22
    },
    "ceval-valid_civil_servant": {
      "original": 47,
      "effective": 47
    },
    "ceval-valid_sports_science": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_plant_protection": {
      "original": 22,
      "effective": 22
    },
    "ceval-valid_basic_medicine": {
      "original": 19,
      "effective": 19
    },
    "ceval-valid_clinical_medicine": {
      "original": 22,
      "effective": 22
    },
    "ceval-valid_urban_and_rural_planner": {
      "original": 46,
      "effective": 46
    },
    "ceval-valid_accountant": {
      "original": 49,
      "effective": 49
    },
    "ceval-valid_fire_engineer": {
      "original": 31,
      "effective": 31
    },
    "ceval-valid_environmental_impact_assessment_engineer": {
      "original": 31,
      "effective": 31
    },
    "ceval-valid_tax_accountant": {
      "original": 49,
      "effective": 49
    },
    "ceval-valid_physician": {
      "original": 49,
      "effective": 49
    },
    "aclue_ancient_chinese_culture": {
      "original": 136,
      "effective": 136
    },
    "aclue_ancient_literature": {
      "original": 160,
      "effective": 160
    },
    "aclue_ancient_medical": {
      "original": 211,
      "effective": 211
    },
    "aclue_ancient_phonetics": {
      "original": 100,
      "effective": 100
    },
    "aclue_basic_ancient_chinese": {
      "original": 249,
      "effective": 249
    },
    "aclue_couplet_prediction": {
      "original": 500,
      "effective": 500
    },
    "aclue_homographic_character_resolution": {
      "original": 500,
      "effective": 500
    },
    "aclue_named_entity_recognition": {
      "original": 500,
      "effective": 500
    },
    "aclue_poetry_appreciate": {
      "original": 103,
      "effective": 103
    },
    "aclue_poetry_context_prediction": {
      "original": 500,
      "effective": 500
    },
    "aclue_poetry_quality_assessment": {
      "original": 406,
      "effective": 406
    },
    "aclue_poetry_sentiment_analysis": {
      "original": 500,
      "effective": 500
    },
    "aclue_polysemy_resolution": {
      "original": 500,
      "effective": 500
    },
    "aclue_reading_comprehension": {
      "original": 101,
      "effective": 101
    },
    "aclue_sentence_segmentation": {
      "original": 500,
      "effective": 500
    }
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=./model/ms/Qwen/Qwen2.5-7B,trust_remote_code=True",
    "model_num_parameters": 7615616512,
    "model_dtype": "torch.bfloat16",
    "model_revision": "main",
    "model_sha": "",
    "batch_size": "4",
    "batch_sizes": [],
    "device": null,
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000,
    "gen_kwargs": null,
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": "2b5b6a3",
  "date": 1754362256.2713373,
  "pretty_env_info": "PyTorch version: 2.7.1+cu126\nIs debug build: False\nCUDA used to build PyTorch: 12.6\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.5 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: version 3.22.1\nLibc version: glibc-2.35\n\nPython version: 3.12.11 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:09:17) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-94-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.8.93\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4090 D\nNvidia driver version: 570.124.04\ncuDNN version: Probably one of the following:\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.8.0\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.8.0\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      46 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             192\nOn-line CPU(s) list:                0-191\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8481C\nCPU family:                         6\nModel:                              143\nThread(s) per core:                 2\nCore(s) per socket:                 48\nSocket(s):                          2\nStepping:                           8\nFrequency boost:                    enabled\nCPU max MHz:                        2201.0000\nCPU min MHz:                        800.0000\nBogoMIPS:                           4400.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                     VT-x\nL1d cache:                          4.5 MiB (96 instances)\nL1i cache:                          3 MiB (96 instances)\nL2 cache:                           192 MiB (96 instances)\nL3 cache:                           210 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0-47,96-143\nNUMA node1 CPU(s):                  48-95,144-191\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==2.3.2\n[pip3] nvidia-cublas-cu12==12.6.4.1\n[pip3] nvidia-cuda-cupti-cu12==12.6.80\n[pip3] nvidia-cuda-nvrtc-cu12==12.6.77\n[pip3] nvidia-cuda-runtime-cu12==12.6.77\n[pip3] nvidia-cudnn-cu12==9.5.1.17\n[pip3] nvidia-cufft-cu12==11.3.0.4\n[pip3] nvidia-curand-cu12==10.3.7.77\n[pip3] nvidia-cusolver-cu12==11.7.1.2\n[pip3] nvidia-cusparse-cu12==12.5.4.2\n[pip3] nvidia-cusparselt-cu12==0.6.3\n[pip3] nvidia-nccl-cu12==2.26.2\n[pip3] nvidia-nvjitlink-cu12==12.6.85\n[pip3] nvidia-nvtx-cu12==12.6.77\n[pip3] torch==2.7.1\n[pip3] triton==3.3.1\n[conda] numpy                     2.3.2                    pypi_0    pypi\n[conda] nvidia-cublas-cu12        12.6.4.1                 pypi_0    pypi\n[conda] nvidia-cuda-cupti-cu12    12.6.80                  pypi_0    pypi\n[conda] nvidia-cuda-nvrtc-cu12    12.6.77                  pypi_0    pypi\n[conda] nvidia-cuda-runtime-cu12  12.6.77                  pypi_0    pypi\n[conda] nvidia-cudnn-cu12         9.5.1.17                 pypi_0    pypi\n[conda] nvidia-cufft-cu12         11.3.0.4                 pypi_0    pypi\n[conda] nvidia-curand-cu12        10.3.7.77                pypi_0    pypi\n[conda] nvidia-cusolver-cu12      11.7.1.2                 pypi_0    pypi\n[conda] nvidia-cusparse-cu12      12.5.4.2                 pypi_0    pypi\n[conda] nvidia-cusparselt-cu12    0.6.3                    pypi_0    pypi\n[conda] nvidia-nccl-cu12          2.26.2                   pypi_0    pypi\n[conda] nvidia-nvjitlink-cu12     12.6.85                  pypi_0    pypi\n[conda] nvidia-nvtx-cu12          12.6.77                  pypi_0    pypi\n[conda] torch                     2.7.1                    pypi_0    pypi\n[conda] triton                    3.3.1                    pypi_0    pypi",
  "transformers_version": "4.54.1",
  "lm_eval_version": "0.4.9",
  "upper_git_hash": null,
  "tokenizer_pad_token": [
    "<|endoftext|>",
    "151643"
  ],
  "tokenizer_eos_token": [
    "<|endoftext|>",
    "151643"
  ],
  "tokenizer_bos_token": [
    null,
    "None"
  ],
  "eot_token_id": 151643,
  "max_length": 131072,
  "task_hashes": {},
  "model_source": "hf",
  "model_name": "./model/ms/Qwen/Qwen2.5-7B",
  "model_name_sanitized": ".__model__ms__Qwen__Qwen2.5-7B",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": false,
  "chat_template": "{%- if tools %}\n    {{- '<|im_start|>system\\n' }}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- messages[0]['content'] }}\n    {%- else %}\n        {{- 'You are a helpful assistant.' }}\n    {%- endif %}\n    {{- \"\\n\\n# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n    {%- for tool in tools %}\n        {{- \"\\n\" }}\n        {{- tool | tojson }}\n    {%- endfor %}\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n{%- else %}\n    {%- if messages[0]['role'] == 'system' %}\n        {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n    {%- else %}\n        {{- '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}\n    {%- endif %}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- elif message.role == \"assistant\" %}\n        {{- '<|im_start|>' + message.role }}\n        {%- if message.content %}\n            {{- '\\n' + message.content }}\n        {%- endif %}\n        {%- for tool_call in message.tool_calls %}\n            {%- if tool_call.function is defined %}\n                {%- set tool_call = tool_call.function %}\n            {%- endif %}\n            {{- '\\n<tool_call>\\n{\"name\": \"' }}\n            {{- tool_call.name }}\n            {{- '\", \"arguments\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- '}\\n</tool_call>' }}\n        {%- endfor %}\n        {{- '<|im_end|>\\n' }}\n    {%- elif message.role == \"tool\" %}\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\n            {{- '<|im_start|>user' }}\n        {%- endif %}\n        {{- '\\n<tool_response>\\n' }}\n        {{- message.content }}\n        {{- '\\n</tool_response>' }}\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n            {{- '<|im_end|>\\n' }}\n        {%- endif %}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n",
  "chat_template_sha": "44d5f08f3f72b837eaad09f13a54c1f9f4eb58d75240334548b7fd52a5437fa5",
  "start_time": 13187322.180667877,
  "end_time": 13188364.07620182,
  "total_evaluation_time_seconds": "1041.8955339435488"
}